{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课：双层神经网络\n",
    "\n",
    "讲师\n",
    "- 褚则伟 \n",
    "- [homepage](http://people.cs.uchicago.edu/~zeweichu/)\n",
    "- [email](zeweichu@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目录\n",
    "- 什么是PyTorch?\n",
    "- Tensor基础\n",
    "- 使用numpy搭建双层神经网络\n",
    "- 使用PyTorch搭建双层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1、什么是PyTorch?\n",
    "\n",
    "PyTorch是一个基于Python的科学计算库，它有以下特点:\n",
    "\n",
    "- 类似于NumPy，但是它可以使用GPU\n",
    "- 可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Tensor基础\n",
    "\n",
    "Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function                    # 为了兼容python2\n",
    "import torch                                             # 需要提前安装torch：https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1）Tensor 创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5x3矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4490e-39, 1.0194e-38, 9.0919e-39],\n",
      "        [8.4490e-39, 1.0745e-38, 1.0102e-38],\n",
      "        [9.6429e-39, 8.9082e-39, 9.6429e-39],\n",
      "        [1.0102e-38, 1.0194e-38, 1.0561e-38],\n",
      "        [1.0469e-38, 9.2756e-39, 4.2246e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)                                    # 这里是随机的，相当于占位符，不是“空的”\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4821, 0.3854, 0.8517],\n",
      "        [0.7962, 0.0632, 0.5409],\n",
      "        [0.8891, 0.6112, 0.7829],\n",
      "        [0.0715, 0.8069, 0.2608],\n",
      "        [0.3292, 0.0119, 0.2759]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)                                     # 矩阵中的元素满足[0,1)的均匀分布\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全部为0，类型为long的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)                  # 也可以 x = torch.zeros(2,3,3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接直接构建tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.4793, -2.4772,  0.9738],\n",
      "        [ 2.0328,  1.3981,  1.7509],\n",
      "        [-0.7931, -0.0291, -0.6803],\n",
      "        [-1.2944, -0.7352, -0.9346],\n",
      "        [ 0.5917, -0.5149, -1.8149]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes,there is no '.ones()' in pytorch\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size\n",
    "                                              # dtype is needed，'randn' but not 'rand'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())                              # ``torch.Size`` 返回的是一个tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）Tensor 操作\n",
    "\n",
    "\n",
    "有很多种tensor运算。我们先介绍加法运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0281, 1.8720, 1.9095],\n",
      "        [1.3242, 1.6702, 1.5501],\n",
      "        [1.5275, 1.8434, 1.7097],\n",
      "        [1.8846, 1.1496, 1.8425],\n",
      "        [1.3632, 1.7262, 1.0422]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double) \n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种着加法的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7113, -1.5490,  1.4009],\n",
      "        [ 2.4590,  1.6504,  2.6889],\n",
      "        [-0.3609,  0.4950, -0.3357],\n",
      "        [-0.5029, -0.3086, -0.1498],\n",
      "        [ 1.2850, -0.3189, -0.8868]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7113, -1.5490,  1.4009],\n",
      "        [ 2.4590,  1.6504,  2.6889],\n",
      "        [-0.3609,  0.4950, -0.3357],\n",
      "        [-0.5029, -0.3086, -0.1498],\n",
      "        [ 1.2850, -0.3189, -0.8868]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7113, -1.5490,  1.4009],\n",
      "        [ 2.4590,  1.6504,  2.6889],\n",
      "        [-0.3609,  0.4950, -0.3357],\n",
      "        [-0.5029, -0.3086, -0.1498],\n",
      "        [ 1.2850, -0.3189, -0.8868]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)                              # 任何in-place的运算都会以``_``结尾。举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4772,  1.3981, -0.0291, -0.7352, -0.5149])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用``torch.view``："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)                                  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用``.item()``方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4726])\n",
      "0.4726296067237854\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多阅读**\n",
    "\n",
    "\n",
    "  各种Tensor operations, 包括transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers在\n",
    "  `<https://pytorch.org/docs/torch>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）Numpy和Tensor之间的转化\n",
    "\n",
    "\n",
    "在Torch Tensor和NumPy array之间相互转化非常容易。\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)                                                    # 注意这里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)                                              # 注意pytorch没有ones操作\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）CUDA Tensors\n",
    "\n",
    "使用``.to``方法，Tensor可以被移动到别的device上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 热身: 用numpy实现两层神经网络\n",
    "\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- $h=W_1*x+b$\n",
    "- $h\\_ReLU = max\\{0,h\\}$\n",
    "- $\\hat{y} = W_2*h\\_ReLU$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)                                   # 64 * 1000\n",
    "y = np.random.randn(N, D_out)                                  # 64 * 10\n",
    "\n",
    "# Randomly initialize weights                   \n",
    "w1 = np.random.randn(D_in, H)                                  # 1000 * 100\n",
    "w2 = np.random.randn(H, D_out)                                 # 100 * 10\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)                                              # 64 * 100\n",
    "    h_relu = np.maximum(h, 0)                                  # 64 * 100\n",
    "    y_pred = h_relu.dot(w2)                                    # 64 * 10\n",
    "\n",
    "    # Compute and print loss(sum but not mean to simplify the calculation)\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    # 参考：https://www.cnblogs.com/pinard/p/10750718.html\n",
    "    loss_grad_y_pred = 2.0 * (y_pred - y)                                             \n",
    "    loss_grad_w2 = h_relu.T.dot(loss_grad_y_pred)               \n",
    "    loss_grad_h_relu = loss_grad_y_pred.dot(w2.T)\n",
    "    loss_grad_h = loss_grad_h_relu.copy()\n",
    "    loss_grad_h[h < 0] = 0\n",
    "    loss_grad_w1 = x.T.dot(loss_grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * loss_grad_w1\n",
    "    w2 -= learning_rate * loss_grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用PyTorch实现前向神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1）PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31704728.0\n",
      "1 25331164.0\n",
      "2 22378086.0\n",
      "3 19262238.0\n",
      "4 15348289.0\n",
      "5 11017595.0\n",
      "6 7356282.0\n",
      "7 4705923.5\n",
      "8 3027346.5\n",
      "9 2012536.375\n",
      "10 1409662.25\n",
      "11 1041771.75\n",
      "12 807321.0625\n",
      "13 649262.0\n",
      "14 536533.1875\n",
      "15 451980.875\n",
      "16 385983.53125\n",
      "17 332925.53125\n",
      "18 289368.1875\n",
      "19 253030.78125\n",
      "20 222354.703125\n",
      "21 196214.3125\n",
      "22 173766.515625\n",
      "23 154378.140625\n",
      "24 137539.375\n",
      "25 122867.1015625\n",
      "26 110037.3515625\n",
      "27 98769.4921875\n",
      "28 88842.109375\n",
      "29 80063.15625\n",
      "30 72279.015625\n",
      "31 65361.66796875\n",
      "32 59195.42578125\n",
      "33 53687.4453125\n",
      "34 48757.57421875\n",
      "35 44338.4453125\n",
      "36 40370.34765625\n",
      "37 36803.1484375\n",
      "38 33587.4453125\n",
      "39 30684.1640625\n",
      "40 28059.435546875\n",
      "41 25683.255859375\n",
      "42 23528.814453125\n",
      "43 21570.8515625\n",
      "44 19792.4296875\n",
      "45 18175.244140625\n",
      "46 16704.6640625\n",
      "47 15364.2578125\n",
      "48 14141.7509765625\n",
      "49 13026.609375\n",
      "50 12007.3115234375\n",
      "51 11075.3896484375\n",
      "52 10221.8857421875\n",
      "53 9439.876953125\n",
      "54 8722.13671875\n",
      "55 8063.46826171875\n",
      "56 7458.20703125\n",
      "57 6901.8876953125\n",
      "58 6390.34375\n",
      "59 5919.4794921875\n",
      "60 5485.79345703125\n",
      "61 5086.119140625\n",
      "62 4718.2138671875\n",
      "63 4378.970703125\n",
      "64 4065.92578125\n",
      "65 3776.7900390625\n",
      "66 3509.54296875\n",
      "67 3262.43408203125\n",
      "68 3033.942626953125\n",
      "69 2822.52490234375\n",
      "70 2627.182373046875\n",
      "71 2446.365966796875\n",
      "72 2278.8046875\n",
      "73 2123.408447265625\n",
      "74 1979.00146484375\n",
      "75 1845.013427734375\n",
      "76 1720.6822509765625\n",
      "77 1605.2548828125\n",
      "78 1498.001953125\n",
      "79 1398.356201171875\n",
      "80 1305.7220458984375\n",
      "81 1219.5579833984375\n",
      "82 1139.3939208984375\n",
      "83 1064.7841796875\n",
      "84 995.3250732421875\n",
      "85 930.6298217773438\n",
      "86 870.3472900390625\n",
      "87 814.1729125976562\n",
      "88 761.8153686523438\n",
      "89 713.0128784179688\n",
      "90 667.50048828125\n",
      "91 625.0264892578125\n",
      "92 585.3772583007812\n",
      "93 548.3762817382812\n",
      "94 513.8129272460938\n",
      "95 481.5259094238281\n",
      "96 451.376708984375\n",
      "97 423.1982116699219\n",
      "98 396.865234375\n",
      "99 372.23583984375\n",
      "100 349.208984375\n",
      "101 327.65960693359375\n",
      "102 307.49652099609375\n",
      "103 288.6243591308594\n",
      "104 270.9569396972656\n",
      "105 254.41790771484375\n",
      "106 238.9322052001953\n",
      "107 224.42202758789062\n",
      "108 210.82664489746094\n",
      "109 198.08383178710938\n",
      "110 186.14157104492188\n",
      "111 174.94784545898438\n",
      "112 164.45217895507812\n",
      "113 154.6090850830078\n",
      "114 145.38900756835938\n",
      "115 136.7398681640625\n",
      "116 128.62008666992188\n",
      "117 121.001708984375\n",
      "118 113.84794616699219\n",
      "119 107.13176727294922\n",
      "120 100.82424926757812\n",
      "121 94.90043640136719\n",
      "122 89.33421325683594\n",
      "123 84.10637664794922\n",
      "124 79.19412994384766\n",
      "125 74.57848358154297\n",
      "126 70.23960876464844\n",
      "127 66.15946197509766\n",
      "128 62.32460403442383\n",
      "129 58.7183723449707\n",
      "130 55.32723617553711\n",
      "131 52.13628387451172\n",
      "132 49.13447570800781\n",
      "133 46.310585021972656\n",
      "134 43.65383529663086\n",
      "135 41.152828216552734\n",
      "136 38.799072265625\n",
      "137 36.583656311035156\n",
      "138 34.49782943725586\n",
      "139 32.53558349609375\n",
      "140 30.6860294342041\n",
      "141 28.94465446472168\n",
      "142 27.304447174072266\n",
      "143 25.759523391723633\n",
      "144 24.304840087890625\n",
      "145 22.93392562866211\n",
      "146 21.641254425048828\n",
      "147 20.42369842529297\n",
      "148 19.276079177856445\n",
      "149 18.194564819335938\n",
      "150 17.175493240356445\n",
      "151 16.214174270629883\n",
      "152 15.308029174804688\n",
      "153 14.454139709472656\n",
      "154 13.648143768310547\n",
      "155 12.88845157623291\n",
      "156 12.171833038330078\n",
      "157 11.49567699432373\n",
      "158 10.85841178894043\n",
      "159 10.256678581237793\n",
      "160 9.689424514770508\n",
      "161 9.154097557067871\n",
      "162 8.64884090423584\n",
      "163 8.172189712524414\n",
      "164 7.721974849700928\n",
      "165 7.297136306762695\n",
      "166 6.8962836265563965\n",
      "167 6.5177459716796875\n",
      "168 6.160311698913574\n",
      "169 5.822811126708984\n",
      "170 5.5043110847473145\n",
      "171 5.203525066375732\n",
      "172 4.919389724731445\n",
      "173 4.651163101196289\n",
      "174 4.3978190422058105\n",
      "175 4.158350944519043\n",
      "176 3.9322471618652344\n",
      "177 3.718606948852539\n",
      "178 3.516770839691162\n",
      "179 3.3262054920196533\n",
      "180 3.1460940837860107\n",
      "181 2.975762367248535\n",
      "182 2.814879894256592\n",
      "183 2.662900447845459\n",
      "184 2.5192079544067383\n",
      "185 2.3834681510925293\n",
      "186 2.255030393600464\n",
      "187 2.13377046585083\n",
      "188 2.0190846920013428\n",
      "189 1.9105591773986816\n",
      "190 1.807981014251709\n",
      "191 1.7110538482666016\n",
      "192 1.6193859577178955\n",
      "193 1.5326906442642212\n",
      "194 1.4506947994232178\n",
      "195 1.373248815536499\n",
      "196 1.2998838424682617\n",
      "197 1.2304624319076538\n",
      "198 1.1650127172470093\n",
      "199 1.1028441190719604\n",
      "200 1.0442299842834473\n",
      "201 0.9886825084686279\n",
      "202 0.9362077713012695\n",
      "203 0.8864397406578064\n",
      "204 0.8394078016281128\n",
      "205 0.7948980927467346\n",
      "206 0.7528337836265564\n",
      "207 0.7129263281822205\n",
      "208 0.6751680374145508\n",
      "209 0.6395058035850525\n",
      "210 0.6058014035224915\n",
      "211 0.573722243309021\n",
      "212 0.5434805750846863\n",
      "213 0.5148582458496094\n",
      "214 0.48777079582214355\n",
      "215 0.462094783782959\n",
      "216 0.4378334879875183\n",
      "217 0.41474175453186035\n",
      "218 0.3928961455821991\n",
      "219 0.37232136726379395\n",
      "220 0.35279765725135803\n",
      "221 0.3343387842178345\n",
      "222 0.31676602363586426\n",
      "223 0.3001691997051239\n",
      "224 0.2844657897949219\n",
      "225 0.26963645219802856\n",
      "226 0.2555326223373413\n",
      "227 0.24219271540641785\n",
      "228 0.22961300611495972\n",
      "229 0.21758520603179932\n",
      "230 0.20622654259204865\n",
      "231 0.19550156593322754\n",
      "232 0.18533945083618164\n",
      "233 0.17566744983196259\n",
      "234 0.16653285920619965\n",
      "235 0.15787597000598907\n",
      "236 0.14970409870147705\n",
      "237 0.14190873503684998\n",
      "238 0.13456779718399048\n",
      "239 0.12759016454219818\n",
      "240 0.12096268683671951\n",
      "241 0.11470359563827515\n",
      "242 0.1087842658162117\n",
      "243 0.10314527899026871\n",
      "244 0.09780357778072357\n",
      "245 0.09277193248271942\n",
      "246 0.08799058943986893\n",
      "247 0.0834306925535202\n",
      "248 0.07912513613700867\n",
      "249 0.0750374048948288\n",
      "250 0.07118058204650879\n",
      "251 0.06751800328493118\n",
      "252 0.06403960287570953\n",
      "253 0.06074457988142967\n",
      "254 0.05762597173452377\n",
      "255 0.05466882884502411\n",
      "256 0.0518682561814785\n",
      "257 0.04920265078544617\n",
      "258 0.04668186977505684\n",
      "259 0.044272683560848236\n",
      "260 0.042025547474622726\n",
      "261 0.03986666351556778\n",
      "262 0.037817493081092834\n",
      "263 0.03588436171412468\n",
      "264 0.03405837342143059\n",
      "265 0.03232688084244728\n",
      "266 0.030674563720822334\n",
      "267 0.029108863323926926\n",
      "268 0.027641309425234795\n",
      "269 0.026221055537462234\n",
      "270 0.024893635883927345\n",
      "271 0.02361663617193699\n",
      "272 0.022424183785915375\n",
      "273 0.02127956785261631\n",
      "274 0.020195599645376205\n",
      "275 0.019174542278051376\n",
      "276 0.01821214333176613\n",
      "277 0.01728914864361286\n",
      "278 0.016413141041994095\n",
      "279 0.01559178251773119\n",
      "280 0.014809946529567242\n",
      "281 0.014066735282540321\n",
      "282 0.01335320807993412\n",
      "283 0.012697878293693066\n",
      "284 0.012057363986968994\n",
      "285 0.011450453661382198\n",
      "286 0.010880804620683193\n",
      "287 0.01034202054142952\n",
      "288 0.009831000119447708\n",
      "289 0.009346149861812592\n",
      "290 0.008878068067133427\n",
      "291 0.00844407919794321\n",
      "292 0.008024066686630249\n",
      "293 0.007635605521500111\n",
      "294 0.0072587537579238415\n",
      "295 0.0069105857983231544\n",
      "296 0.006573254242539406\n",
      "297 0.006256978493183851\n",
      "298 0.005949943792074919\n",
      "299 0.005672339349985123\n",
      "300 0.005388857331126928\n",
      "301 0.0051320018246769905\n",
      "302 0.004887753631919622\n",
      "303 0.004658843856304884\n",
      "304 0.0044357734732329845\n",
      "305 0.004228176549077034\n",
      "306 0.004027842078357935\n",
      "307 0.003840153571218252\n",
      "308 0.0036594069097191095\n",
      "309 0.003490033093839884\n",
      "310 0.0033292584121227264\n",
      "311 0.0031785538885742426\n",
      "312 0.003031808650121093\n",
      "313 0.002896031131967902\n",
      "314 0.0027637507300823927\n",
      "315 0.002640662482008338\n",
      "316 0.0025206280406564474\n",
      "317 0.0024077417328953743\n",
      "318 0.0022998738568276167\n",
      "319 0.0022006493527442217\n",
      "320 0.002101228339597583\n",
      "321 0.0020116977393627167\n",
      "322 0.0019245940493419766\n",
      "323 0.0018393839709460735\n",
      "324 0.0017626716289669275\n",
      "325 0.001689193886704743\n",
      "326 0.0016162614338099957\n",
      "327 0.0015509161166846752\n",
      "328 0.0014848458813503385\n",
      "329 0.0014197597047314048\n",
      "330 0.0013633108465000987\n",
      "331 0.0013077231124043465\n",
      "332 0.001255737035535276\n",
      "333 0.001203768653795123\n",
      "334 0.0011556316167116165\n",
      "335 0.001109623583033681\n",
      "336 0.0010652983328327537\n",
      "337 0.0010259364498779178\n",
      "338 0.0009847141336649656\n",
      "339 0.0009464982431381941\n",
      "340 0.0009106658981181681\n",
      "341 0.0008753696456551552\n",
      "342 0.0008441813406534493\n",
      "343 0.0008131045615300536\n",
      "344 0.0007834337884560227\n",
      "345 0.0007538509089499712\n",
      "346 0.0007265112362802029\n",
      "347 0.0007019559852778912\n",
      "348 0.0006759824464097619\n",
      "349 0.0006510045495815575\n",
      "350 0.0006284148548729718\n",
      "351 0.0006068204529583454\n",
      "352 0.0005856421194039285\n",
      "353 0.0005672844126820564\n",
      "354 0.0005473798955790699\n",
      "355 0.0005280547775328159\n",
      "356 0.0005113428342156112\n",
      "357 0.0004943141248077154\n",
      "358 0.00047874540905468166\n",
      "359 0.00046168401604518294\n",
      "360 0.000447523663751781\n",
      "361 0.0004326198832131922\n",
      "362 0.00041988512384705245\n",
      "363 0.00040688799344934523\n",
      "364 0.0003942836483474821\n",
      "365 0.000381028454285115\n",
      "366 0.0003701212117448449\n",
      "367 0.00035913955071009696\n",
      "368 0.0003480427258182317\n",
      "369 0.00033798906952142715\n",
      "370 0.00032894761534407735\n",
      "371 0.0003196335455868393\n",
      "372 0.0003099186287727207\n",
      "373 0.0003019550640601665\n",
      "374 0.00029274728149175644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.0002844816190190613\n",
      "376 0.00027625024085864425\n",
      "377 0.0002687727683223784\n",
      "378 0.0002608516369946301\n",
      "379 0.00025311342324130237\n",
      "380 0.0002469048195052892\n",
      "381 0.00024049097555689514\n",
      "382 0.0002342124644201249\n",
      "383 0.00022811403323430568\n",
      "384 0.00022231723414734006\n",
      "385 0.0002166029589716345\n",
      "386 0.00021077181736472994\n",
      "387 0.00020510501053649932\n",
      "388 0.00020020001102238894\n",
      "389 0.0001948442222783342\n",
      "390 0.00018990584067068994\n",
      "391 0.00018529882072471082\n",
      "392 0.00018070911755785346\n",
      "393 0.00017650797963142395\n",
      "394 0.00017214834224432707\n",
      "395 0.0001683011942077428\n",
      "396 0.00016451899136882275\n",
      "397 0.00016050187696237117\n",
      "398 0.00015686434926465154\n",
      "399 0.00015321985119953752\n",
      "400 0.0001501761726103723\n",
      "401 0.00014639270375482738\n",
      "402 0.00014274154091253877\n",
      "403 0.0001396275474689901\n",
      "404 0.0001364489580737427\n",
      "405 0.00013346801279112697\n",
      "406 0.00013024920190218836\n",
      "407 0.00012755846546497196\n",
      "408 0.00012532222899608314\n",
      "409 0.0001224723382620141\n",
      "410 0.00011974618973908946\n",
      "411 0.00011740042100427672\n",
      "412 0.00011441943206591532\n",
      "413 0.00011229746451135725\n",
      "414 0.00010995937191182747\n",
      "415 0.00010784588812384754\n",
      "416 0.00010610915342113003\n",
      "417 0.0001038835325744003\n",
      "418 0.00010166718857362866\n",
      "419 9.979418973671272e-05\n",
      "420 9.793229401111603e-05\n",
      "421 9.590695117367432e-05\n",
      "422 9.412408689968288e-05\n",
      "423 9.244915418094024e-05\n",
      "424 9.07004505279474e-05\n",
      "425 8.880807581590489e-05\n",
      "426 8.733373397262767e-05\n",
      "427 8.574980893172324e-05\n",
      "428 8.392545714741573e-05\n",
      "429 8.241042814916e-05\n",
      "430 8.080529369181022e-05\n",
      "431 7.939618808450177e-05\n",
      "432 7.762608584016562e-05\n",
      "433 7.651503256056458e-05\n",
      "434 7.531026494689286e-05\n",
      "435 7.377369183814153e-05\n",
      "436 7.305829058168456e-05\n",
      "437 7.153345359256491e-05\n",
      "438 7.028930122032762e-05\n",
      "439 6.921228487044573e-05\n",
      "440 6.803637370467186e-05\n",
      "441 6.695889896946028e-05\n",
      "442 6.582081550732255e-05\n",
      "443 6.459224823629484e-05\n",
      "444 6.373634096235037e-05\n",
      "445 6.257549830479547e-05\n",
      "446 6.140403274912387e-05\n",
      "447 6.0855145420646295e-05\n",
      "448 5.961475471849553e-05\n",
      "449 5.8864923630608246e-05\n",
      "450 5.767813490820117e-05\n",
      "451 5.6913944717962295e-05\n",
      "452 5.6101172958733514e-05\n",
      "453 5.5124517530202866e-05\n",
      "454 5.3974403272150084e-05\n",
      "455 5.3289859351934865e-05\n",
      "456 5.267193409963511e-05\n",
      "457 5.193992910790257e-05\n",
      "458 5.1057362725259736e-05\n",
      "459 5.001332101528533e-05\n",
      "460 4.934622847940773e-05\n",
      "461 4.873232319368981e-05\n",
      "462 4.794801498064771e-05\n",
      "463 4.7256213292712346e-05\n",
      "464 4.667185203288682e-05\n",
      "465 4.5966684410814196e-05\n",
      "466 4.526913107838482e-05\n",
      "467 4.486504985834472e-05\n",
      "468 4.413699934957549e-05\n",
      "469 4.358588557806797e-05\n",
      "470 4.305447146180086e-05\n",
      "471 4.2635525460354984e-05\n",
      "472 4.186580190435052e-05\n",
      "473 4.1199065890396014e-05\n",
      "474 4.055891258758493e-05\n",
      "475 4.017409810330719e-05\n",
      "476 3.963488052249886e-05\n",
      "477 3.913479667971842e-05\n",
      "478 3.8683563616359606e-05\n",
      "479 3.806965833064169e-05\n",
      "480 3.7681969843106344e-05\n",
      "481 3.737308725249022e-05\n",
      "482 3.669063517008908e-05\n",
      "483 3.630801438703202e-05\n",
      "484 3.584063597372733e-05\n",
      "485 3.539228782756254e-05\n",
      "486 3.4903492633020505e-05\n",
      "487 3.4551478165667504e-05\n",
      "488 3.4130087442463264e-05\n",
      "489 3.377102984813973e-05\n",
      "490 3.320474206702784e-05\n",
      "491 3.283058322267607e-05\n",
      "492 3.254006151109934e-05\n",
      "493 3.1907922675600275e-05\n",
      "494 3.1705902074463665e-05\n",
      "495 3.147914321743883e-05\n",
      "496 3.111985279247165e-05\n",
      "497 3.079450470977463e-05\n",
      "498 3.0240607884479687e-05\n",
      "499 2.9828101105522364e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    # print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2) PyTorch: Tensor和autograd\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30642318.0\n",
      "1 26655638.0\n",
      "2 23372702.0\n",
      "3 18951928.0\n",
      "4 13784028.0\n",
      "5 9170854.0\n",
      "6 5822788.0\n",
      "7 3723261.5\n",
      "8 2486424.25\n",
      "9 1766218.75\n",
      "10 1331391.0\n",
      "11 1053403.375\n",
      "12 862462.375\n",
      "13 722628.8125\n",
      "14 614955.5\n",
      "15 528794.6875\n",
      "16 458177.625\n",
      "17 399186.0625\n",
      "18 349321.09375\n",
      "19 306805.28125\n",
      "20 270350.8125\n",
      "21 238968.0625\n",
      "22 211810.15625\n",
      "23 188173.53125\n",
      "24 167540.3125\n",
      "25 149474.046875\n",
      "26 133631.046875\n",
      "27 119707.4921875\n",
      "28 107422.0625\n",
      "29 96554.7890625\n",
      "30 86918.8359375\n",
      "31 78362.078125\n",
      "32 70763.1171875\n",
      "33 63986.265625\n",
      "34 57931.71484375\n",
      "35 52509.37890625\n",
      "36 47647.5078125\n",
      "37 43286.98828125\n",
      "38 39369.26953125\n",
      "39 35842.5859375\n",
      "40 32663.876953125\n",
      "41 29794.2578125\n",
      "42 27200.099609375\n",
      "43 24853.697265625\n",
      "44 22731.751953125\n",
      "45 20807.77734375\n",
      "46 19061.01171875\n",
      "47 17473.966796875\n",
      "48 16031.42578125\n",
      "49 14717.21875\n",
      "50 13519.9296875\n",
      "51 12427.6630859375\n",
      "52 11430.892578125\n",
      "53 10521.0419921875\n",
      "54 9690.3408203125\n",
      "55 8930.34765625\n",
      "56 8234.3701171875\n",
      "57 7596.94775390625\n",
      "58 7011.93896484375\n",
      "59 6475.44970703125\n",
      "60 5983.0576171875\n",
      "61 5530.43017578125\n",
      "62 5114.3076171875\n",
      "63 4731.5927734375\n",
      "64 4379.41455078125\n",
      "65 4054.998291015625\n",
      "66 3756.000244140625\n",
      "67 3480.38525390625\n",
      "68 3226.28369140625\n",
      "69 2991.737060546875\n",
      "70 2775.318603515625\n",
      "71 2575.5625\n",
      "72 2391.017578125\n",
      "73 2220.281494140625\n",
      "74 2062.343994140625\n",
      "75 1916.2674560546875\n",
      "76 1781.0799560546875\n",
      "77 1655.8927001953125\n",
      "78 1539.89990234375\n",
      "79 1432.4732666015625\n",
      "80 1332.912353515625\n",
      "81 1240.6070556640625\n",
      "82 1154.9417724609375\n",
      "83 1075.4588623046875\n",
      "84 1001.699951171875\n",
      "85 933.2359619140625\n",
      "86 869.672119140625\n",
      "87 810.6102294921875\n",
      "88 755.7183227539062\n",
      "89 704.69921875\n",
      "90 657.2651977539062\n",
      "91 613.16455078125\n",
      "92 572.1558227539062\n",
      "93 533.992919921875\n",
      "94 498.4564208984375\n",
      "95 465.3757019042969\n",
      "96 434.5774230957031\n",
      "97 405.89544677734375\n",
      "98 379.17791748046875\n",
      "99 354.2796630859375\n",
      "100 331.0776062011719\n",
      "101 309.46234130859375\n",
      "102 289.29962158203125\n",
      "103 270.4910888671875\n",
      "104 252.95501708984375\n",
      "105 236.62326049804688\n",
      "106 221.39613342285156\n",
      "107 207.1868438720703\n",
      "108 193.91818237304688\n",
      "109 181.53292846679688\n",
      "110 169.9641876220703\n",
      "111 159.15545654296875\n",
      "112 149.05874633789062\n",
      "113 139.6233367919922\n",
      "114 130.8057098388672\n",
      "115 122.56584167480469\n",
      "116 114.85669708251953\n",
      "117 107.6517333984375\n",
      "118 100.9100112915039\n",
      "119 94.6060562133789\n",
      "120 88.70616149902344\n",
      "121 83.18428802490234\n",
      "122 78.01892852783203\n",
      "123 73.18209075927734\n",
      "124 68.65693664550781\n",
      "125 64.41938781738281\n",
      "126 60.451637268066406\n",
      "127 56.735496520996094\n",
      "128 53.25367736816406\n",
      "129 49.99188995361328\n",
      "130 46.933475494384766\n",
      "131 44.069374084472656\n",
      "132 41.385433197021484\n",
      "133 38.86858367919922\n",
      "134 36.508636474609375\n",
      "135 34.29573059082031\n",
      "136 32.22139358520508\n",
      "137 30.275768280029297\n",
      "138 28.450084686279297\n",
      "139 26.73849105834961\n",
      "140 25.132532119750977\n",
      "141 23.62497329711914\n",
      "142 22.210752487182617\n",
      "143 20.883190155029297\n",
      "144 19.63550567626953\n",
      "145 18.465299606323242\n",
      "146 17.366853713989258\n",
      "147 16.335113525390625\n",
      "148 15.36551284790039\n",
      "149 14.455304145812988\n",
      "150 13.600530624389648\n",
      "151 12.797468185424805\n",
      "152 12.043266296386719\n",
      "153 11.334388732910156\n",
      "154 10.667970657348633\n",
      "155 10.041535377502441\n",
      "156 9.452908515930176\n",
      "157 8.900179862976074\n",
      "158 8.379827499389648\n",
      "159 7.891049385070801\n",
      "160 7.430540084838867\n",
      "161 6.998000621795654\n",
      "162 6.591617584228516\n",
      "163 6.209443092346191\n",
      "164 5.849093437194824\n",
      "165 5.510497570037842\n",
      "166 5.192013740539551\n",
      "167 4.892485618591309\n",
      "168 4.61035680770874\n",
      "169 4.344936847686768\n",
      "170 4.09501838684082\n",
      "171 3.8599348068237305\n",
      "172 3.638709545135498\n",
      "173 3.43001127243042\n",
      "174 3.2339978218078613\n",
      "175 3.049208402633667\n",
      "176 2.8751916885375977\n",
      "177 2.7112643718719482\n",
      "178 2.55692982673645\n",
      "179 2.4113521575927734\n",
      "180 2.2746808528900146\n",
      "181 2.1456620693206787\n",
      "182 2.023982286453247\n",
      "183 1.9095126390457153\n",
      "184 1.801648736000061\n",
      "185 1.699906349182129\n",
      "186 1.6039758920669556\n",
      "187 1.5135581493377686\n",
      "188 1.4285099506378174\n",
      "189 1.3483697175979614\n",
      "190 1.2725261449813843\n",
      "191 1.2012546062469482\n",
      "192 1.1339973211288452\n",
      "193 1.0704656839370728\n",
      "194 1.0106925964355469\n",
      "195 0.9542280435562134\n",
      "196 0.901037335395813\n",
      "197 0.8508431911468506\n",
      "198 0.8034790754318237\n",
      "199 0.7588093280792236\n",
      "200 0.7166969180107117\n",
      "201 0.676956057548523\n",
      "202 0.6394277811050415\n",
      "203 0.6040104627609253\n",
      "204 0.5705671906471252\n",
      "205 0.5390437245368958\n",
      "206 0.5093144178390503\n",
      "207 0.48117735981941223\n",
      "208 0.45473167300224304\n",
      "209 0.4297017455101013\n",
      "210 0.4060324728488922\n",
      "211 0.3837348222732544\n",
      "212 0.36266401410102844\n",
      "213 0.34278416633605957\n",
      "214 0.3240605592727661\n",
      "215 0.3062954843044281\n",
      "216 0.2895369529724121\n",
      "217 0.2737630605697632\n",
      "218 0.2588649094104767\n",
      "219 0.24463370442390442\n",
      "220 0.23141035437583923\n",
      "221 0.2187301516532898\n",
      "222 0.20687361061573029\n",
      "223 0.1956217885017395\n",
      "224 0.18503662943840027\n",
      "225 0.1749928593635559\n",
      "226 0.16550223529338837\n",
      "227 0.1565399467945099\n",
      "228 0.14805394411087036\n",
      "229 0.1400892436504364\n",
      "230 0.13254159688949585\n",
      "231 0.125328928232193\n",
      "232 0.11859367042779922\n",
      "233 0.1122085377573967\n",
      "234 0.10618442296981812\n",
      "235 0.10045348107814789\n",
      "236 0.09504395723342896\n",
      "237 0.08995167911052704\n",
      "238 0.08512596786022186\n",
      "239 0.08057911694049835\n",
      "240 0.07626080513000488\n",
      "241 0.07219067960977554\n",
      "242 0.06831738352775574\n",
      "243 0.06466760486364365\n",
      "244 0.06121611222624779\n",
      "245 0.057955145835876465\n",
      "246 0.054834723472595215\n",
      "247 0.05193934589624405\n",
      "248 0.04916832223534584\n",
      "249 0.04656647890806198\n",
      "250 0.04409509897232056\n",
      "251 0.04173779487609863\n",
      "252 0.03952088952064514\n",
      "253 0.037436965852975845\n",
      "254 0.03544077277183533\n",
      "255 0.03355942666530609\n",
      "256 0.031800154596567154\n",
      "257 0.03011816181242466\n",
      "258 0.02854391746222973\n",
      "259 0.027045413851737976\n",
      "260 0.02562529966235161\n",
      "261 0.02428814396262169\n",
      "262 0.023001810535788536\n",
      "263 0.02179950848221779\n",
      "264 0.02065836638212204\n",
      "265 0.019582070410251617\n",
      "266 0.018553797155618668\n",
      "267 0.01757689379155636\n",
      "268 0.016666265204548836\n",
      "269 0.015796978026628494\n",
      "270 0.014979067258536816\n",
      "271 0.014213381335139275\n",
      "272 0.013477049767971039\n",
      "273 0.012784862890839577\n",
      "274 0.012119228020310402\n",
      "275 0.011496366932988167\n",
      "276 0.010909082368016243\n",
      "277 0.010345370508730412\n",
      "278 0.009816593490540981\n",
      "279 0.009318441152572632\n",
      "280 0.008840450085699558\n",
      "281 0.008394819684326649\n",
      "282 0.007970788516104221\n",
      "283 0.007564548868685961\n",
      "284 0.0071852304972708225\n",
      "285 0.0068220579996705055\n",
      "286 0.006479697301983833\n",
      "287 0.006155646871775389\n",
      "288 0.005847014952450991\n",
      "289 0.005559494718909264\n",
      "290 0.0052851964719593525\n",
      "291 0.005020905286073685\n",
      "292 0.004774055443704128\n",
      "293 0.004545706789940596\n",
      "294 0.004328018054366112\n",
      "295 0.004116611555218697\n",
      "296 0.00391670037060976\n",
      "297 0.003730078926309943\n",
      "298 0.0035493881441652775\n",
      "299 0.003382486291229725\n",
      "300 0.0032247386407107115\n",
      "301 0.0030718441121280193\n",
      "302 0.002931711496785283\n",
      "303 0.0027993572875857353\n",
      "304 0.002673991257324815\n",
      "305 0.002547793323174119\n",
      "306 0.0024286287371069193\n",
      "307 0.002318543614819646\n",
      "308 0.002212385181337595\n",
      "309 0.0021163870114833117\n",
      "310 0.002019918290898204\n",
      "311 0.0019298532279208302\n",
      "312 0.0018447161419317126\n",
      "313 0.001764299813657999\n",
      "314 0.001688638818450272\n",
      "315 0.0016157530480995774\n",
      "316 0.0015480124857276678\n",
      "317 0.0014824883546680212\n",
      "318 0.001420467859134078\n",
      "319 0.0013631015317514539\n",
      "320 0.0013071463909000158\n",
      "321 0.0012530552921816707\n",
      "322 0.0012017684057354927\n",
      "323 0.001150435651652515\n",
      "324 0.0011065286817029119\n",
      "325 0.0010603194823488593\n",
      "326 0.0010199827374890447\n",
      "327 0.000980167998932302\n",
      "328 0.0009413756197318435\n",
      "329 0.0009053170215338469\n",
      "330 0.0008714956929907203\n",
      "331 0.000837055966258049\n",
      "332 0.0008046440780162811\n",
      "333 0.0007751055527478456\n",
      "334 0.0007458826876245439\n",
      "335 0.0007195392390713096\n",
      "336 0.0006939327577129006\n",
      "337 0.0006668911082670093\n",
      "338 0.0006438801065087318\n",
      "339 0.0006200342322699726\n",
      "340 0.0005972086219117045\n",
      "341 0.0005764549132436514\n",
      "342 0.0005570763605646789\n",
      "343 0.0005379561916925013\n",
      "344 0.0005197733407840133\n",
      "345 0.000503181538078934\n",
      "346 0.00048589170910418034\n",
      "347 0.00046962167834863067\n",
      "348 0.0004536268243100494\n",
      "349 0.00043972407002002\n",
      "350 0.00042636191938072443\n",
      "351 0.00041223131120204926\n",
      "352 0.0003992853162344545\n",
      "353 0.00038694575778208673\n",
      "354 0.0003748764283955097\n",
      "355 0.00036313594318926334\n",
      "356 0.00035213123192079365\n",
      "357 0.00034143266384489834\n",
      "358 0.0003308911109343171\n",
      "359 0.0003209034912288189\n",
      "360 0.0003118129388894886\n",
      "361 0.0003027828352060169\n",
      "362 0.00029435456963256\n",
      "363 0.00028598110657185316\n",
      "364 0.0002779096248559654\n",
      "365 0.0002693138085305691\n",
      "366 0.0002623187901917845\n",
      "367 0.0002556524414103478\n",
      "368 0.00024846504675224423\n",
      "369 0.00024124571064021438\n",
      "370 0.00023578957188874483\n",
      "371 0.00022927881218492985\n",
      "372 0.0002238260640297085\n",
      "373 0.00021757406648248434\n",
      "374 0.00021203253709245473\n",
      "375 0.00020616830443032086\n",
      "376 0.00020140122796874493\n",
      "377 0.00019608964794315398\n",
      "378 0.00019178001093678176\n",
      "379 0.000186426768777892\n",
      "380 0.00018168652604799718\n",
      "381 0.00017717311857268214\n",
      "382 0.00017348724941257387\n",
      "383 0.00016913827857933939\n",
      "384 0.00016494795272592455\n",
      "385 0.0001608129241503775\n",
      "386 0.0001570331078255549\n",
      "387 0.00015366863226518035\n",
      "388 0.00015007385809440166\n",
      "389 0.00014710547111462802\n",
      "390 0.00014341308269649744\n",
      "391 0.00014061862020753324\n",
      "392 0.0001374081475660205\n",
      "393 0.0001344756456092\n",
      "394 0.0001316052657784894\n",
      "395 0.0001292011875193566\n",
      "396 0.00012650237476918846\n",
      "397 0.00012362685811240226\n",
      "398 0.00012095629790564999\n",
      "399 0.00011843194079119712\n",
      "400 0.00011599875142564997\n",
      "401 0.00011340586206642911\n",
      "402 0.00011100868141511455\n",
      "403 0.00010897816537180915\n",
      "404 0.00010690608905861154\n",
      "405 0.00010455059236846864\n",
      "406 0.00010281874710926786\n",
      "407 0.00010074975580209866\n",
      "408 9.869763016467914e-05\n",
      "409 9.672641317592934e-05\n",
      "410 9.441263682674617e-05\n",
      "411 9.299361408920959e-05\n",
      "412 9.076092828763649e-05\n",
      "413 8.89522343641147e-05\n",
      "414 8.758043259149417e-05\n",
      "415 8.60959553392604e-05\n",
      "416 8.427166903857142e-05\n",
      "417 8.30264762043953e-05\n",
      "418 8.141125726979226e-05\n",
      "419 8.003493712749332e-05\n",
      "420 7.840728358132765e-05\n",
      "421 7.721139263594523e-05\n",
      "422 7.585048297187313e-05\n",
      "423 7.471536810044199e-05\n",
      "424 7.339912554016337e-05\n",
      "425 7.216075027827173e-05\n",
      "426 7.091643055900931e-05\n",
      "427 6.979042518651113e-05\n",
      "428 6.833407678641379e-05\n",
      "429 6.72917376505211e-05\n",
      "430 6.644415407208726e-05\n",
      "431 6.494114495581016e-05\n",
      "432 6.39036952634342e-05\n",
      "433 6.281322566792369e-05\n",
      "434 6.203277735039592e-05\n",
      "435 6.0951890191063285e-05\n",
      "436 5.989443525322713e-05\n",
      "437 5.887847873964347e-05\n",
      "438 5.8012159570353106e-05\n",
      "439 5.7047654991038144e-05\n",
      "440 5.580670404015109e-05\n",
      "441 5.5110431276261806e-05\n",
      "442 5.427981523098424e-05\n",
      "443 5.345633690012619e-05\n",
      "444 5.251108086667955e-05\n",
      "445 5.1864783017663285e-05\n",
      "446 5.1160437578801066e-05\n",
      "447 5.027716906624846e-05\n",
      "448 4.966429332853295e-05\n",
      "449 4.909660492558032e-05\n",
      "450 4.8676523874746636e-05\n",
      "451 4.79152295156382e-05\n",
      "452 4.7297420678660274e-05\n",
      "453 4.664231528295204e-05\n",
      "454 4.589202217175625e-05\n",
      "455 4.550872108666226e-05\n",
      "456 4.485028694034554e-05\n",
      "457 4.415847797645256e-05\n",
      "458 4.3482559703988954e-05\n",
      "459 4.290501237846911e-05\n",
      "460 4.224172516842373e-05\n",
      "461 4.167628867435269e-05\n",
      "462 4.113350951229222e-05\n",
      "463 4.05798273277469e-05\n",
      "464 3.9895949157653376e-05\n",
      "465 3.939038288081065e-05\n",
      "466 3.8964215491432697e-05\n",
      "467 3.8474427128676325e-05\n",
      "468 3.800563354161568e-05\n",
      "469 3.7463374610524625e-05\n",
      "470 3.716276478371583e-05\n",
      "471 3.656071930890903e-05\n",
      "472 3.629878119681962e-05\n",
      "473 3.593420115066692e-05\n",
      "474 3.539434328558855e-05\n",
      "475 3.492604082566686e-05\n",
      "476 3.4526659874245524e-05\n",
      "477 3.408914926694706e-05\n",
      "478 3.371727143530734e-05\n",
      "479 3.334831853862852e-05\n",
      "480 3.283220939920284e-05\n",
      "481 3.248659777455032e-05\n",
      "482 3.197445403202437e-05\n",
      "483 3.173263758071698e-05\n",
      "484 3.142674177070148e-05\n",
      "485 3.111417026957497e-05\n",
      "486 3.0727373086847365e-05\n",
      "487 3.0413566491915844e-05\n",
      "488 3.0046419851714745e-05\n",
      "489 2.9757446100120433e-05\n",
      "490 2.941277671197895e-05\n",
      "491 2.9089689633110538e-05\n",
      "492 2.8870495953015052e-05\n",
      "493 2.8692536943708546e-05\n",
      "494 2.84534598904429e-05\n",
      "495 2.82178425550228e-05\n",
      "496 2.790360667859204e-05\n",
      "497 2.7717762350221165e-05\n",
      "498 2.7495972972246818e-05\n",
      "499 2.719703843467869e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N 是 batch size; D_in 是 input dimension;\n",
    "# H 是 hidden dimension; D_out 是 output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机的Tensor来保存输入和输出\n",
    "# 设定requires_grad=False表示在反向传播的时候我们不需要计算gradient\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 创建随机的Tensor和权重。\n",
    "# 设置requires_grad=True表示我们希望反向传播的时候计算Tensor的gradient\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播:通过Tensor预测y；这个和普通的神经网络的前向传播没有任何不同，\n",
    "    # 但是我们不需要保存网络的中间运算结果，因为我们不需要手动计算反向传播。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # 通过前向传播计算loss\n",
    "    # loss是一个形状为(1，)的Tensor\n",
    "    # loss.item()可以给我们返回一个loss的scalar\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    # print(t, loss.item())\n",
    "\n",
    "    # PyTorch给我们提供了autograd的方法做反向传播。如果一个Tensor的requires_grad=True，\n",
    "    # backward会自动计算loss相对于每个Tensor的gradient。在backward之后，\n",
    "    # w1.grad和w2.grad会包含两个loss相对于两个Tensor的gradient信息。\n",
    "    loss.backward()\n",
    "\n",
    "    # 我们可以手动做gradient descent(后面我们会介绍自动的方法)。\n",
    "    # \n",
    "    # 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，\n",
    "    # 但是在更新weights之后（w1 -= learning_rate * w1.grad 和 w2 -= learning_rate * w2.grad 这里）我们并不需要再做autograd。\n",
    "    # \n",
    "    # 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。\n",
    "    # tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，\n",
    "    # 但是不会记录计算图的历史。\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # 否则每次更新会自动叠加之前的grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3) PyTorch: nn\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 616.8349609375\n",
      "1 570.4186401367188\n",
      "2 530.6421508789062\n",
      "3 495.7164001464844\n",
      "4 464.91497802734375\n",
      "5 437.1092834472656\n",
      "6 411.87066650390625\n",
      "7 388.5781555175781\n",
      "8 367.105224609375\n",
      "9 347.06768798828125\n",
      "10 328.3486328125\n",
      "11 310.6429748535156\n",
      "12 294.08880615234375\n",
      "13 278.54046630859375\n",
      "14 263.8558044433594\n",
      "15 249.83802795410156\n",
      "16 236.52313232421875\n",
      "17 223.8170166015625\n",
      "18 211.7015380859375\n",
      "19 200.13755798339844\n",
      "20 189.1465301513672\n",
      "21 178.6802520751953\n",
      "22 168.74122619628906\n",
      "23 159.31674194335938\n",
      "24 150.35125732421875\n",
      "25 141.79025268554688\n",
      "26 133.63401794433594\n",
      "27 125.89380645751953\n",
      "28 118.53340148925781\n",
      "29 111.54275512695312\n",
      "30 104.91582489013672\n",
      "31 98.65790557861328\n",
      "32 92.7421646118164\n",
      "33 87.18020629882812\n",
      "34 81.94192504882812\n",
      "35 77.01036834716797\n",
      "36 72.3639144897461\n",
      "37 67.99095916748047\n",
      "38 63.88977813720703\n",
      "39 60.036468505859375\n",
      "40 56.426231384277344\n",
      "41 53.05012512207031\n",
      "42 49.88925552368164\n",
      "43 46.92338943481445\n",
      "44 44.14652633666992\n",
      "45 41.54481887817383\n",
      "46 39.10710144042969\n",
      "47 36.83108139038086\n",
      "48 34.69960403442383\n",
      "49 32.706153869628906\n",
      "50 30.837730407714844\n",
      "51 29.088411331176758\n",
      "52 27.445863723754883\n",
      "53 25.909109115600586\n",
      "54 24.46878433227539\n",
      "55 23.117572784423828\n",
      "56 21.849687576293945\n",
      "57 20.658483505249023\n",
      "58 19.538114547729492\n",
      "59 18.48484992980957\n",
      "60 17.494647979736328\n",
      "61 16.563005447387695\n",
      "62 15.686758995056152\n",
      "63 14.856837272644043\n",
      "64 14.07450008392334\n",
      "65 13.338075637817383\n",
      "66 12.644659042358398\n",
      "67 11.991114616394043\n",
      "68 11.375090599060059\n",
      "69 10.79391098022461\n",
      "70 10.2453031539917\n",
      "71 9.728269577026367\n",
      "72 9.24122142791748\n",
      "73 8.78144359588623\n",
      "74 8.347025871276855\n",
      "75 7.936522960662842\n",
      "76 7.547698497772217\n",
      "77 7.180155277252197\n",
      "78 6.832812786102295\n",
      "79 6.503701210021973\n",
      "80 6.192763328552246\n",
      "81 5.897329330444336\n",
      "82 5.617427349090576\n",
      "83 5.3520426750183105\n",
      "84 5.100094795227051\n",
      "85 4.860849380493164\n",
      "86 4.634103298187256\n",
      "87 4.419088363647461\n",
      "88 4.2145843505859375\n",
      "89 4.020376682281494\n",
      "90 3.8357491493225098\n",
      "91 3.660381317138672\n",
      "92 3.493623733520508\n",
      "93 3.3348333835601807\n",
      "94 3.1840157508850098\n",
      "95 3.040458917617798\n",
      "96 2.90377140045166\n",
      "97 2.7736332416534424\n",
      "98 2.6498191356658936\n",
      "99 2.531902313232422\n",
      "100 2.419586658477783\n",
      "101 2.312699317932129\n",
      "102 2.2109007835388184\n",
      "103 2.113172769546509\n",
      "104 2.0200531482696533\n",
      "105 1.931264042854309\n",
      "106 1.8465205430984497\n",
      "107 1.765866994857788\n",
      "108 1.6889408826828003\n",
      "109 1.6155205965042114\n",
      "110 1.545505404472351\n",
      "111 1.478671669960022\n",
      "112 1.4148930311203003\n",
      "113 1.354078769683838\n",
      "114 1.2960002422332764\n",
      "115 1.2405637502670288\n",
      "116 1.1877013444900513\n",
      "117 1.1373679637908936\n",
      "118 1.0893242359161377\n",
      "119 1.0433975458145142\n",
      "120 0.9995244741439819\n",
      "121 0.957534909248352\n",
      "122 0.9174259901046753\n",
      "123 0.8791078329086304\n",
      "124 0.8424510359764099\n",
      "125 0.8073628544807434\n",
      "126 0.7738264203071594\n",
      "127 0.7417508959770203\n",
      "128 0.7110787630081177\n",
      "129 0.6817481517791748\n",
      "130 0.6536678075790405\n",
      "131 0.6267886161804199\n",
      "132 0.6010832786560059\n",
      "133 0.5764719843864441\n",
      "134 0.55291348695755\n",
      "135 0.5303621292114258\n",
      "136 0.508778989315033\n",
      "137 0.4881012439727783\n",
      "138 0.46830493211746216\n",
      "139 0.4493491053581238\n",
      "140 0.4311933219432831\n",
      "141 0.41380244493484497\n",
      "142 0.3971446752548218\n",
      "143 0.3811851739883423\n",
      "144 0.3658958375453949\n",
      "145 0.35125595331192017\n",
      "146 0.33723005652427673\n",
      "147 0.3237687349319458\n",
      "148 0.3108976483345032\n",
      "149 0.2985706031322479\n",
      "150 0.28673914074897766\n",
      "151 0.27540862560272217\n",
      "152 0.26455509662628174\n",
      "153 0.2541574537754059\n",
      "154 0.24418404698371887\n",
      "155 0.23461730778217316\n",
      "156 0.2254522293806076\n",
      "157 0.2166563868522644\n",
      "158 0.2082248032093048\n",
      "159 0.20014120638370514\n",
      "160 0.1923948973417282\n",
      "161 0.18496574461460114\n",
      "162 0.17784026265144348\n",
      "163 0.17100077867507935\n",
      "164 0.16445767879486084\n",
      "165 0.15817493200302124\n",
      "166 0.1521458625793457\n",
      "167 0.14637260138988495\n",
      "168 0.14082613587379456\n",
      "169 0.13550066947937012\n",
      "170 0.1303861439228058\n",
      "171 0.12548044323921204\n",
      "172 0.12077119201421738\n",
      "173 0.11624855548143387\n",
      "174 0.11190006136894226\n",
      "175 0.10772517323493958\n",
      "176 0.10371194034814835\n",
      "177 0.0998566597700119\n",
      "178 0.09615585952997208\n",
      "179 0.09260128438472748\n",
      "180 0.08918008953332901\n",
      "181 0.08589056134223938\n",
      "182 0.08273126929998398\n",
      "183 0.07969754189252853\n",
      "184 0.076775923371315\n",
      "185 0.07396624237298965\n",
      "186 0.07126504927873611\n",
      "187 0.0686689168214798\n",
      "188 0.06617266684770584\n",
      "189 0.06377072632312775\n",
      "190 0.06145996227860451\n",
      "191 0.05924048647284508\n",
      "192 0.057103026658296585\n",
      "193 0.055055245757102966\n",
      "194 0.05308816209435463\n",
      "195 0.05119483917951584\n",
      "196 0.04937274008989334\n",
      "197 0.04762033745646477\n",
      "198 0.04593187943100929\n",
      "199 0.044306349009275436\n",
      "200 0.04274118319153786\n",
      "201 0.04123516380786896\n",
      "202 0.03978639841079712\n",
      "203 0.038389310240745544\n",
      "204 0.03704371303319931\n",
      "205 0.0357489138841629\n",
      "206 0.03450245037674904\n",
      "207 0.03330256789922714\n",
      "208 0.0321439690887928\n",
      "209 0.03102947771549225\n",
      "210 0.02995586395263672\n",
      "211 0.02892051823437214\n",
      "212 0.02792329527437687\n",
      "213 0.026961468160152435\n",
      "214 0.02603522501885891\n",
      "215 0.02514214999973774\n",
      "216 0.02428179606795311\n",
      "217 0.02345244586467743\n",
      "218 0.02265304885804653\n",
      "219 0.021882878616452217\n",
      "220 0.02113926038146019\n",
      "221 0.02042245678603649\n",
      "222 0.019731855019927025\n",
      "223 0.01906573213636875\n",
      "224 0.018423302099108696\n",
      "225 0.017803329974412918\n",
      "226 0.017205584794282913\n",
      "227 0.016628772020339966\n",
      "228 0.016072314232587814\n",
      "229 0.015535356476902962\n",
      "230 0.015017733909189701\n",
      "231 0.014518306590616703\n",
      "232 0.014036747626960278\n",
      "233 0.013571222312748432\n",
      "234 0.013121938332915306\n",
      "235 0.012688718736171722\n",
      "236 0.012270272709429264\n",
      "237 0.011866560205817223\n",
      "238 0.011476823128759861\n",
      "239 0.011100317351520061\n",
      "240 0.010736910626292229\n",
      "241 0.010386170819401741\n",
      "242 0.0100497892126441\n",
      "243 0.0097251171246171\n",
      "244 0.009411685168743134\n",
      "245 0.009108913131058216\n",
      "246 0.008816596120595932\n",
      "247 0.00853422749787569\n",
      "248 0.008261235430836678\n",
      "249 0.007997557520866394\n",
      "250 0.007742919027805328\n",
      "251 0.007496801670640707\n",
      "252 0.007259095553308725\n",
      "253 0.007029606495052576\n",
      "254 0.006807629019021988\n",
      "255 0.006593053694814444\n",
      "256 0.006385627202689648\n",
      "257 0.006185163743793964\n",
      "258 0.005991355516016483\n",
      "259 0.005803895648568869\n",
      "260 0.005622652359306812\n",
      "261 0.0054474519565701485\n",
      "262 0.005278183612972498\n",
      "263 0.005114411935210228\n",
      "264 0.004955946933478117\n",
      "265 0.004802867770195007\n",
      "266 0.004654645454138517\n",
      "267 0.004511530976742506\n",
      "268 0.004373411647975445\n",
      "269 0.00423995777964592\n",
      "270 0.004110764712095261\n",
      "271 0.003985690884292126\n",
      "272 0.003864638740196824\n",
      "273 0.0037474501878023148\n",
      "274 0.0036340728402137756\n",
      "275 0.0035244303289800882\n",
      "276 0.0034184216056019068\n",
      "277 0.0033156615681946278\n",
      "278 0.0032162207644432783\n",
      "279 0.003119939938187599\n",
      "280 0.0030267902184277773\n",
      "281 0.0029364691581577063\n",
      "282 0.002849065698683262\n",
      "283 0.00276445085182786\n",
      "284 0.0026824434753507376\n",
      "285 0.0026030712760984898\n",
      "286 0.002526269294321537\n",
      "287 0.002451810520142317\n",
      "288 0.0023796753957867622\n",
      "289 0.0023097810335457325\n",
      "290 0.002242131158709526\n",
      "291 0.002176533453166485\n",
      "292 0.002112946705892682\n",
      "293 0.002051342511549592\n",
      "294 0.0019916510209441185\n",
      "295 0.0019338340498507023\n",
      "296 0.0018778254743665457\n",
      "297 0.0018235259922221303\n",
      "298 0.0017708562081679702\n",
      "299 0.001719821011647582\n",
      "300 0.001670365920290351\n",
      "301 0.0016224351711571217\n",
      "302 0.001575930742546916\n",
      "303 0.0015308443689718843\n",
      "304 0.0014871100429445505\n",
      "305 0.0014447338180616498\n",
      "306 0.0014036260545253754\n",
      "307 0.0013637744123116136\n",
      "308 0.0013251130003482103\n",
      "309 0.00128761469386518\n",
      "310 0.00125124619808048\n",
      "311 0.0012159458128735423\n",
      "312 0.0011817403137683868\n",
      "313 0.0011485485592857003\n",
      "314 0.0011163217714056373\n",
      "315 0.0010850488906726241\n",
      "316 0.0010547296842560172\n",
      "317 0.0010253143263980746\n",
      "318 0.0009967893129214644\n",
      "319 0.0009690661099739373\n",
      "320 0.0009421803988516331\n",
      "321 0.0009160566260106862\n",
      "322 0.0008907159208320081\n",
      "323 0.0008661439060233533\n",
      "324 0.0008422775426879525\n",
      "325 0.0008190966327674687\n",
      "326 0.0007965879631228745\n",
      "327 0.0007747435010969639\n",
      "328 0.000753526808694005\n",
      "329 0.0007329185027629137\n",
      "330 0.000712935405317694\n",
      "331 0.0006935214041732252\n",
      "332 0.0006746658473275602\n",
      "333 0.0006563455681316555\n",
      "334 0.0006385522428900003\n",
      "335 0.000621272309217602\n",
      "336 0.0006044972687959671\n",
      "337 0.0005881927208974957\n",
      "338 0.00057236134307459\n",
      "339 0.0005569689092226326\n",
      "340 0.0005420160596258938\n",
      "341 0.000527493713889271\n",
      "342 0.000513377774041146\n",
      "343 0.0004996666684746742\n",
      "344 0.0004863426147494465\n",
      "345 0.00047339097363874316\n",
      "346 0.0004608244926203042\n",
      "347 0.0004486078687477857\n",
      "348 0.0004367205547168851\n",
      "349 0.0004251683712936938\n",
      "350 0.0004139319353271276\n",
      "351 0.0004030133131891489\n",
      "352 0.00039240249316208065\n",
      "353 0.00038208605838008225\n",
      "354 0.00037205807166174054\n",
      "355 0.0003623157390393317\n",
      "356 0.0003528340021148324\n",
      "357 0.00034362293081358075\n",
      "358 0.0003346629673615098\n",
      "359 0.0003259552177041769\n",
      "360 0.0003174722078256309\n",
      "361 0.00030922345467843115\n",
      "362 0.0003012036031577736\n",
      "363 0.00029340494074858725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 0.00028583104722201824\n",
      "365 0.000278460793197155\n",
      "366 0.00027128090732730925\n",
      "367 0.00026430474827066064\n",
      "368 0.0002575131948105991\n",
      "369 0.0002509095938876271\n",
      "370 0.00024448230396956205\n",
      "371 0.00023822381626814604\n",
      "372 0.0002321432693861425\n",
      "373 0.00022622810502070934\n",
      "374 0.0002204657648690045\n",
      "375 0.0002148632047465071\n",
      "376 0.00020940121612511575\n",
      "377 0.00020409838180057704\n",
      "378 0.0001989272132050246\n",
      "379 0.00019389843509998173\n",
      "380 0.00018900231225416064\n",
      "381 0.0001842392230173573\n",
      "382 0.00017960301192943007\n",
      "383 0.00017508988094050437\n",
      "384 0.00017069902969524264\n",
      "385 0.00016641429101582617\n",
      "386 0.00016225305444095284\n",
      "387 0.000158198265125975\n",
      "388 0.00015424926823470742\n",
      "389 0.000150404914165847\n",
      "390 0.0001466635148972273\n",
      "391 0.00014301779447123408\n",
      "392 0.00013946628314442933\n",
      "393 0.00013601550017483532\n",
      "394 0.00013265143206808716\n",
      "395 0.00012937198334839195\n",
      "396 0.00012617645552381873\n",
      "397 0.0001230676716659218\n",
      "398 0.0001200390252051875\n",
      "399 0.00011708753299899399\n",
      "400 0.00011421682575019076\n",
      "401 0.00011141804861836135\n",
      "402 0.00010869121615542099\n",
      "403 0.00010603333066683263\n",
      "404 0.00010344652400817722\n",
      "405 0.00010092253796756268\n",
      "406 9.846295870374888e-05\n",
      "407 9.6071045845747e-05\n",
      "408 9.373520879307762e-05\n",
      "409 9.146131196757779e-05\n",
      "410 8.924482244765386e-05\n",
      "411 8.70852600201033e-05\n",
      "412 8.498283568769693e-05\n",
      "413 8.293260907521471e-05\n",
      "414 8.093572250800207e-05\n",
      "415 7.89878613431938e-05\n",
      "416 7.708741031819955e-05\n",
      "417 7.523671229137108e-05\n",
      "418 7.34319764887914e-05\n",
      "419 7.167273724917322e-05\n",
      "420 6.995951844146475e-05\n",
      "421 6.828828190919012e-05\n",
      "422 6.665829278063029e-05\n",
      "423 6.507106445496902e-05\n",
      "424 6.35203905403614e-05\n",
      "425 6.200941425049677e-05\n",
      "426 6.05389905103948e-05\n",
      "427 5.9100995713379234e-05\n",
      "428 5.7702251069713384e-05\n",
      "429 5.633640830637887e-05\n",
      "430 5.500561746885069e-05\n",
      "431 5.3707120969193056e-05\n",
      "432 5.2441897423705086e-05\n",
      "433 5.1206407079007477e-05\n",
      "434 5.000238525099121e-05\n",
      "435 4.882620123680681e-05\n",
      "436 4.76813547720667e-05\n",
      "437 4.656398596125655e-05\n",
      "438 4.5474393118638545e-05\n",
      "439 4.4412216084310785e-05\n",
      "440 4.3375530367484316e-05\n",
      "441 4.236115637468174e-05\n",
      "442 4.137431824347004e-05\n",
      "443 4.0410945075564086e-05\n",
      "444 3.94726412196178e-05\n",
      "445 3.8555674109375104e-05\n",
      "446 3.765980727621354e-05\n",
      "447 3.6788511351915076e-05\n",
      "448 3.593677683966234e-05\n",
      "449 3.510593160171993e-05\n",
      "450 3.4294465876882896e-05\n",
      "451 3.350316546857357e-05\n",
      "452 3.273158290539868e-05\n",
      "453 3.197851401637308e-05\n",
      "454 3.124314753222279e-05\n",
      "455 3.0524690373567864e-05\n",
      "456 2.982511796290055e-05\n",
      "457 2.9140464903321117e-05\n",
      "458 2.8473716156440787e-05\n",
      "459 2.7822752599604428e-05\n",
      "460 2.718799078138545e-05\n",
      "461 2.656610740814358e-05\n",
      "462 2.5960111088352278e-05\n",
      "463 2.536940883146599e-05\n",
      "464 2.479194699844811e-05\n",
      "465 2.4228789698099717e-05\n",
      "466 2.367901470279321e-05\n",
      "467 2.314150333404541e-05\n",
      "468 2.2617272406932898e-05\n",
      "469 2.2104968593339436e-05\n",
      "470 2.1604977519018576e-05\n",
      "471 2.111716639774386e-05\n",
      "472 2.0640816728700884e-05\n",
      "473 2.0175557438051328e-05\n",
      "474 1.9721246644621715e-05\n",
      "475 1.92775023606373e-05\n",
      "476 1.8843960788217373e-05\n",
      "477 1.8419739717501216e-05\n",
      "478 1.8007833205047064e-05\n",
      "479 1.760409759299364e-05\n",
      "480 1.7209487850777805e-05\n",
      "481 1.6824411432025954e-05\n",
      "482 1.6448399037471972e-05\n",
      "483 1.608097954886034e-05\n",
      "484 1.5722764146630652e-05\n",
      "485 1.537234493298456e-05\n",
      "486 1.5029500900709536e-05\n",
      "487 1.4695562640554272e-05\n",
      "488 1.4368548363563605e-05\n",
      "489 1.4049977835384198e-05\n",
      "490 1.3737889275944326e-05\n",
      "491 1.3433097592496779e-05\n",
      "492 1.3135630069882609e-05\n",
      "493 1.2845604032918345e-05\n",
      "494 1.256068117072573e-05\n",
      "495 1.2283581781957764e-05\n",
      "496 1.2012886145384982e-05\n",
      "497 1.1748711585823912e-05\n",
      "498 1.1490279575809836e-05\n",
      "499 1.1236619684495963e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4) PyTorch: optim\n",
    "\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 595.8619384765625\n",
      "1 579.458740234375\n",
      "2 563.612060546875\n",
      "3 548.3158569335938\n",
      "4 533.6061401367188\n",
      "5 519.296875\n",
      "6 505.4200134277344\n",
      "7 491.98504638671875\n",
      "8 478.9765319824219\n",
      "9 466.3656921386719\n",
      "10 454.0737609863281\n",
      "11 442.1894836425781\n",
      "12 430.68804931640625\n",
      "13 419.524169921875\n",
      "14 408.7554626464844\n",
      "15 398.3657531738281\n",
      "16 388.31939697265625\n",
      "17 378.4873352050781\n",
      "18 368.9092102050781\n",
      "19 359.59173583984375\n",
      "20 350.52880859375\n",
      "21 341.7240295410156\n",
      "22 333.14678955078125\n",
      "23 324.7843322753906\n",
      "24 316.6291809082031\n",
      "25 308.6772155761719\n",
      "26 300.9322509765625\n",
      "27 293.3556823730469\n",
      "28 285.9695129394531\n",
      "29 278.813232421875\n",
      "30 271.8526611328125\n",
      "31 265.1206970214844\n",
      "32 258.5401916503906\n",
      "33 252.11834716796875\n",
      "34 245.84507751464844\n",
      "35 239.69578552246094\n",
      "36 233.69839477539062\n",
      "37 227.81935119628906\n",
      "38 222.08218383789062\n",
      "39 216.4769744873047\n",
      "40 211.0048370361328\n",
      "41 205.65638732910156\n",
      "42 200.43299865722656\n",
      "43 195.33209228515625\n",
      "44 190.35610961914062\n",
      "45 185.51107788085938\n",
      "46 180.78871154785156\n",
      "47 176.17066955566406\n",
      "48 171.63966369628906\n",
      "49 167.21031188964844\n",
      "50 162.88063049316406\n",
      "51 158.64385986328125\n",
      "52 154.51611328125\n",
      "53 150.46994018554688\n",
      "54 146.4930877685547\n",
      "55 142.61825561523438\n",
      "56 138.82803344726562\n",
      "57 135.1127166748047\n",
      "58 131.470703125\n",
      "59 127.9072494506836\n",
      "60 124.41683959960938\n",
      "61 121.00228118896484\n",
      "62 117.6580810546875\n",
      "63 114.38182067871094\n",
      "64 111.18158721923828\n",
      "65 108.05332946777344\n",
      "66 105.0064697265625\n",
      "67 102.03365325927734\n",
      "68 99.12406158447266\n",
      "69 96.28526306152344\n",
      "70 93.51637268066406\n",
      "71 90.8143310546875\n",
      "72 88.17442321777344\n",
      "73 85.60480499267578\n",
      "74 83.09274291992188\n",
      "75 80.64204406738281\n",
      "76 78.24700927734375\n",
      "77 75.91118621826172\n",
      "78 73.63166046142578\n",
      "79 71.40299987792969\n",
      "80 69.22262573242188\n",
      "81 67.09273529052734\n",
      "82 65.01777648925781\n",
      "83 62.99504470825195\n",
      "84 61.02153396606445\n",
      "85 59.09312438964844\n",
      "86 57.21245193481445\n",
      "87 55.381282806396484\n",
      "88 53.59913635253906\n",
      "89 51.8670768737793\n",
      "90 50.18378448486328\n",
      "91 48.545658111572266\n",
      "92 46.94767761230469\n",
      "93 45.393394470214844\n",
      "94 43.88447189331055\n",
      "95 42.41758728027344\n",
      "96 40.988555908203125\n",
      "97 39.59742736816406\n",
      "98 38.243412017822266\n",
      "99 36.9240608215332\n",
      "100 35.64387512207031\n",
      "101 34.402767181396484\n",
      "102 33.19767761230469\n",
      "103 32.0279655456543\n",
      "104 30.891613006591797\n",
      "105 29.790109634399414\n",
      "106 28.71890640258789\n",
      "107 27.681032180786133\n",
      "108 26.675329208374023\n",
      "109 25.699417114257812\n",
      "110 24.75486183166504\n",
      "111 23.840024948120117\n",
      "112 22.953479766845703\n",
      "113 22.09554100036621\n",
      "114 21.26557159423828\n",
      "115 20.46312713623047\n",
      "116 19.68770980834961\n",
      "117 18.939424514770508\n",
      "118 18.21501922607422\n",
      "119 17.51372528076172\n",
      "120 16.836637496948242\n",
      "121 16.183385848999023\n",
      "122 15.550844192504883\n",
      "123 14.940756797790527\n",
      "124 14.351969718933105\n",
      "125 13.782999038696289\n",
      "126 13.234509468078613\n",
      "127 12.705862998962402\n",
      "128 12.195924758911133\n",
      "129 11.70336627960205\n",
      "130 11.228377342224121\n",
      "131 10.77088451385498\n",
      "132 10.329237937927246\n",
      "133 9.904054641723633\n",
      "134 9.49454116821289\n",
      "135 9.100723266601562\n",
      "136 8.72006607055664\n",
      "137 8.353126525878906\n",
      "138 8.000267028808594\n",
      "139 7.660038471221924\n",
      "140 7.332549571990967\n",
      "141 7.017522811889648\n",
      "142 6.715127944946289\n",
      "143 6.424456596374512\n",
      "144 6.145240306854248\n",
      "145 5.876552581787109\n",
      "146 5.618346691131592\n",
      "147 5.370292663574219\n",
      "148 5.132187843322754\n",
      "149 4.903289794921875\n",
      "150 4.683549404144287\n",
      "151 4.472535133361816\n",
      "152 4.270052433013916\n",
      "153 4.075716495513916\n",
      "154 3.889265775680542\n",
      "155 3.7104218006134033\n",
      "156 3.5390172004699707\n",
      "157 3.3747289180755615\n",
      "158 3.2174952030181885\n",
      "159 3.0669217109680176\n",
      "160 2.922604560852051\n",
      "161 2.7844324111938477\n",
      "162 2.6523032188415527\n",
      "163 2.525986909866333\n",
      "164 2.405128240585327\n",
      "165 2.28946852684021\n",
      "166 2.178940773010254\n",
      "167 2.073273181915283\n",
      "168 1.9724122285842896\n",
      "169 1.8761283159255981\n",
      "170 1.784043550491333\n",
      "171 1.695996880531311\n",
      "172 1.6119264364242554\n",
      "173 1.5315971374511719\n",
      "174 1.4549462795257568\n",
      "175 1.3818608522415161\n",
      "176 1.3120875358581543\n",
      "177 1.2455285787582397\n",
      "178 1.182033658027649\n",
      "179 1.121525764465332\n",
      "180 1.0639550685882568\n",
      "181 1.0090988874435425\n",
      "182 0.9568653702735901\n",
      "183 0.9071496725082397\n",
      "184 0.8598688244819641\n",
      "185 0.8148767352104187\n",
      "186 0.7721615433692932\n",
      "187 0.731484591960907\n",
      "188 0.6928466558456421\n",
      "189 0.6561394929885864\n",
      "190 0.6212468147277832\n",
      "191 0.5881820917129517\n",
      "192 0.5567317605018616\n",
      "193 0.5268424153327942\n",
      "194 0.4985887408256531\n",
      "195 0.47185391187667847\n",
      "196 0.4464856684207916\n",
      "197 0.42241838574409485\n",
      "198 0.3995845317840576\n",
      "199 0.3779374659061432\n",
      "200 0.3574145436286926\n",
      "201 0.33796069025993347\n",
      "202 0.3195328712463379\n",
      "203 0.3020438849925995\n",
      "204 0.2854982614517212\n",
      "205 0.26982012391090393\n",
      "206 0.2549697160720825\n",
      "207 0.2409016340970993\n",
      "208 0.2275833636522293\n",
      "209 0.21498295664787292\n",
      "210 0.20305399596691132\n",
      "211 0.19177545607089996\n",
      "212 0.18112324178218842\n",
      "213 0.17104879021644592\n",
      "214 0.16152165830135345\n",
      "215 0.15249817073345184\n",
      "216 0.14397725462913513\n",
      "217 0.13590098917484283\n",
      "218 0.1282719373703003\n",
      "219 0.12105533480644226\n",
      "220 0.11423151940107346\n",
      "221 0.10778035968542099\n",
      "222 0.101686030626297\n",
      "223 0.09591902792453766\n",
      "224 0.0904708281159401\n",
      "225 0.08532275259494781\n",
      "226 0.0804576724767685\n",
      "227 0.07586367428302765\n",
      "228 0.07152118533849716\n",
      "229 0.06742068380117416\n",
      "230 0.06354401260614395\n",
      "231 0.05988387018442154\n",
      "232 0.05642841383814812\n",
      "233 0.05317052826285362\n",
      "234 0.05008852109313011\n",
      "235 0.04718104749917984\n",
      "236 0.04443727806210518\n",
      "237 0.04184841364622116\n",
      "238 0.03940636292099953\n",
      "239 0.037101130932569504\n",
      "240 0.034924570471048355\n",
      "241 0.032872311770915985\n",
      "242 0.030937347561120987\n",
      "243 0.029112311080098152\n",
      "244 0.027390697970986366\n",
      "245 0.02576778270304203\n",
      "246 0.02423822320997715\n",
      "247 0.02279612608253956\n",
      "248 0.02143750712275505\n",
      "249 0.020156294107437134\n",
      "250 0.018949678167700768\n",
      "251 0.01781374402344227\n",
      "252 0.01674306020140648\n",
      "253 0.015733687207102776\n",
      "254 0.014784649945795536\n",
      "255 0.013889944180846214\n",
      "256 0.0130484439432621\n",
      "257 0.012255811132490635\n",
      "258 0.011510089971125126\n",
      "259 0.010808320716023445\n",
      "260 0.010147511959075928\n",
      "261 0.009526554495096207\n",
      "262 0.008941810578107834\n",
      "263 0.008391563780605793\n",
      "264 0.007874293252825737\n",
      "265 0.007388104684650898\n",
      "266 0.006931057199835777\n",
      "267 0.006501136347651482\n",
      "268 0.006097512785345316\n",
      "269 0.005717623978853226\n",
      "270 0.005360812414437532\n",
      "271 0.005025640595704317\n",
      "272 0.004710808862000704\n",
      "273 0.004415055271238089\n",
      "274 0.004137126728892326\n",
      "275 0.0038762155454605818\n",
      "276 0.003631293773651123\n",
      "277 0.0034014012198895216\n",
      "278 0.0031856244895607233\n",
      "279 0.0029830830171704292\n",
      "280 0.00279332441277802\n",
      "281 0.0026148161850869656\n",
      "282 0.002447610953822732\n",
      "283 0.002290663542225957\n",
      "284 0.0021435325033962727\n",
      "285 0.0020056026987731457\n",
      "286 0.0018762737745419145\n",
      "287 0.0017550417687743902\n",
      "288 0.0016413381090387702\n",
      "289 0.0015349077293649316\n",
      "290 0.001435095677152276\n",
      "291 0.001341615803539753\n",
      "292 0.0012540030293166637\n",
      "293 0.0011719950707629323\n",
      "294 0.0010951830772683024\n",
      "295 0.0010232129134237766\n",
      "296 0.0009558862657286227\n",
      "297 0.0008928469032980502\n",
      "298 0.0008338692714460194\n",
      "299 0.0007786231581121683\n",
      "300 0.0007269356865435839\n",
      "301 0.0006786126177757978\n",
      "302 0.0006334199570119381\n",
      "303 0.0005911394255235791\n",
      "304 0.000551586679648608\n",
      "305 0.0005146220792084932\n",
      "306 0.0004800503666047007\n",
      "307 0.0004477481415960938\n",
      "308 0.0004175470385234803\n",
      "309 0.00038934266194701195\n",
      "310 0.00036297738552093506\n",
      "311 0.0003383703588042408\n",
      "312 0.0003153763245791197\n",
      "313 0.00029388838447630405\n",
      "314 0.0002738201874308288\n",
      "315 0.0002551119541749358\n",
      "316 0.00023762315686326474\n",
      "317 0.0002213066618423909\n",
      "318 0.0002060924598481506\n",
      "319 0.0001918906345963478\n",
      "320 0.00017862387176137418\n",
      "321 0.00016626021533738822\n",
      "322 0.00015473057283088565\n",
      "323 0.0001439856132492423\n",
      "324 0.0001339569571428001\n",
      "325 0.00012460775906220078\n",
      "326 0.00011589763016672805\n",
      "327 0.00010777468560263515\n",
      "328 0.00010020661284215748\n",
      "329 9.315714851254597e-05\n",
      "330 8.658706065034494e-05\n",
      "331 8.047871961025521e-05\n",
      "332 7.477628241758794e-05\n",
      "333 6.947012298041955e-05\n",
      "334 6.453145761042833e-05\n",
      "335 5.9933157899649814e-05\n",
      "336 5.565815808949992e-05\n",
      "337 5.167192648514174e-05\n",
      "338 4.79678092233371e-05\n",
      "339 4.452299253898673e-05\n",
      "340 4.1316747228847817e-05\n",
      "341 3.8333513657562435e-05\n",
      "342 3.5561581171350554e-05\n",
      "343 3.298503361293115e-05\n",
      "344 3.058910442632623e-05\n",
      "345 2.836332168953959e-05\n",
      "346 2.6293224436813034e-05\n",
      "347 2.4370727260247804e-05\n",
      "348 2.2583382815355435e-05\n",
      "349 2.0924222553730942e-05\n",
      "350 1.9383131075301208e-05\n",
      "351 1.7954327631741762e-05\n",
      "352 1.6624155250610784e-05\n",
      "353 1.539246477477718e-05\n",
      "354 1.4247690160118509e-05\n",
      "355 1.3186782780394424e-05\n",
      "356 1.2202439393149689e-05\n",
      "357 1.1289610483800061e-05\n",
      "358 1.044231430569198e-05\n",
      "359 9.657024747866672e-06\n",
      "360 8.929513569455594e-06\n",
      "361 8.254161002696492e-06\n",
      "362 7.627957529621199e-06\n",
      "363 7.0496989792445675e-06\n",
      "364 6.512769232358551e-06\n",
      "365 6.015558938088361e-06\n",
      "366 5.555009011004586e-06\n",
      "367 5.127928943693405e-06\n",
      "368 4.733880814455915e-06\n",
      "369 4.369081125332741e-06\n",
      "370 4.030424406664679e-06\n",
      "371 3.7173970213189023e-06\n",
      "372 3.428954414630425e-06\n",
      "373 3.161604809065466e-06\n",
      "374 2.913631078627077e-06\n",
      "375 2.686435436771717e-06\n",
      "376 2.4746150302235037e-06\n",
      "377 2.2803790216130437e-06\n",
      "378 2.099577841363498e-06\n",
      "379 1.9334027001605136e-06\n",
      "380 1.7796619431464933e-06\n",
      "381 1.6376835674236645e-06\n",
      "382 1.5066991636558669e-06\n",
      "383 1.386175995321537e-06\n",
      "384 1.274861801903171e-06\n",
      "385 1.1717243069142569e-06\n",
      "386 1.0773577514555654e-06\n",
      "387 9.897516974888276e-07\n",
      "388 9.092925097320403e-07\n",
      "389 8.351413498530746e-07\n",
      "390 7.667786121601239e-07\n",
      "391 7.036538818283589e-07\n",
      "392 6.456485266426171e-07\n",
      "393 5.927182087361871e-07\n",
      "394 5.43682233455911e-07\n",
      "395 4.98432370932278e-07\n",
      "396 4.5691274408454774e-07\n",
      "397 4.1878399770212127e-07\n",
      "398 3.8345089592439763e-07\n",
      "399 3.5108200791000854e-07\n",
      "400 3.214974810816784e-07\n",
      "401 2.942066998912196e-07\n",
      "402 2.691748477445799e-07\n",
      "403 2.46287271465917e-07\n",
      "404 2.2509161112793663e-07\n",
      "405 2.0588024085554935e-07\n",
      "406 1.8812781377164356e-07\n",
      "407 1.7201743673922465e-07\n",
      "408 1.5695412969307654e-07\n",
      "409 1.4323660479931277e-07\n",
      "410 1.3069508497665083e-07\n",
      "411 1.1929405729915743e-07\n",
      "412 1.0887251988833668e-07\n",
      "413 9.930327848906018e-08\n",
      "414 9.05589843114285e-08\n",
      "415 8.246846761039706e-08\n",
      "416 7.511933830528505e-08\n",
      "417 6.849407441222866e-08\n",
      "418 6.23886009520902e-08\n",
      "419 5.6578436158361e-08\n",
      "420 5.1482327734220235e-08\n",
      "421 4.6790724184120336e-08\n",
      "422 4.253429608525039e-08\n",
      "423 3.8643563016194094e-08\n",
      "424 3.5146676680142264e-08\n",
      "425 3.180745622444192e-08\n",
      "426 2.890652339715416e-08\n",
      "427 2.6270573272313413e-08\n",
      "428 2.3777385393941586e-08\n",
      "429 2.1540916605999882e-08\n",
      "430 1.958386164346848e-08\n",
      "431 1.7749496095120776e-08\n",
      "432 1.6090137222590783e-08\n",
      "433 1.456184417492068e-08\n",
      "434 1.3206699733814276e-08\n",
      "435 1.1926868381806344e-08\n",
      "436 1.0803149486093844e-08\n",
      "437 9.795039446203191e-09\n",
      "438 8.86961881718662e-09\n",
      "439 8.028449016705963e-09\n",
      "440 7.23930826396213e-09\n",
      "441 6.53445164644495e-09\n",
      "442 5.935215874330879e-09\n",
      "443 5.390339730126925e-09\n",
      "444 4.836783862316452e-09\n",
      "445 4.3800727489440305e-09\n",
      "446 3.9429508547073056e-09\n",
      "447 3.5757576899442256e-09\n",
      "448 3.2222577939222674e-09\n",
      "449 2.9230273757008263e-09\n",
      "450 2.648730346166417e-09\n",
      "451 2.4114774621608603e-09\n",
      "452 2.1773620684228945e-09\n",
      "453 1.9813561902282117e-09\n",
      "454 1.7834058674282005e-09\n",
      "455 1.6158866467108624e-09\n",
      "456 1.4695905603545611e-09\n",
      "457 1.3404474197287186e-09\n",
      "458 1.2259246950918623e-09\n",
      "459 1.107040903391976e-09\n",
      "460 1.015665329617832e-09\n",
      "461 9.217280272366679e-10\n",
      "462 8.441080057153272e-10\n",
      "463 7.699606507927115e-10\n",
      "464 7.100589560771198e-10\n",
      "465 6.575374134065726e-10\n",
      "466 6.009081010560635e-10\n",
      "467 5.518029366768928e-10\n",
      "468 5.130821878474023e-10\n",
      "469 4.719287183263532e-10\n",
      "470 4.378121754022857e-10\n",
      "471 4.0654044020094204e-10\n",
      "472 3.7995562252035597e-10\n",
      "473 3.5238381657087814e-10\n",
      "474 3.3464428450535877e-10\n",
      "475 3.1351360396669747e-10\n",
      "476 2.933511766833874e-10\n",
      "477 2.7753255249507447e-10\n",
      "478 2.5709886997127285e-10\n",
      "479 2.441928326213372e-10\n",
      "480 2.3070167998184843e-10\n",
      "481 2.176580859991617e-10\n",
      "482 2.0629355168555463e-10\n",
      "483 1.9692791841663393e-10\n",
      "484 1.846747615941169e-10\n",
      "485 1.7647600047965284e-10\n",
      "486 1.6793615109644833e-10\n",
      "487 1.6097953525751052e-10\n",
      "488 1.541772404189956e-10\n",
      "489 1.4703777084790204e-10\n",
      "490 1.4095050127060915e-10\n",
      "491 1.3943353416312476e-10\n",
      "492 1.3351054983790078e-10\n",
      "493 1.2900518153724505e-10\n",
      "494 1.237693697531128e-10\n",
      "495 1.1670822641640655e-10\n",
      "496 1.1499984992058288e-10\n",
      "497 1.1309382597080031e-10\n",
      "498 1.0735824035323915e-10\n",
      "499 1.0496128965975515e-10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5) PyTorch: 自定义 nn Modules\n",
    "\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 656.6958618164062\n",
      "1 608.1090087890625\n",
      "2 566.172607421875\n",
      "3 529.2335815429688\n",
      "4 496.7382507324219\n",
      "5 467.453125\n",
      "6 440.5755310058594\n",
      "7 416.12872314453125\n",
      "8 393.6068420410156\n",
      "9 372.708251953125\n",
      "10 353.00006103515625\n",
      "11 334.477783203125\n",
      "12 316.97283935546875\n",
      "13 300.36737060546875\n",
      "14 284.6544189453125\n",
      "15 269.65936279296875\n",
      "16 255.33456420898438\n",
      "17 241.66688537597656\n",
      "18 228.60800170898438\n",
      "19 216.09536743164062\n",
      "20 204.13780212402344\n",
      "21 192.75645446777344\n",
      "22 181.89234924316406\n",
      "23 171.58370971679688\n",
      "24 161.7939453125\n",
      "25 152.4780731201172\n",
      "26 143.59371948242188\n",
      "27 135.14727783203125\n",
      "28 127.13992309570312\n",
      "29 119.55585479736328\n",
      "30 112.37797546386719\n",
      "31 105.62073516845703\n",
      "32 99.24383544921875\n",
      "33 93.24134826660156\n",
      "34 87.58341979980469\n",
      "35 82.25212860107422\n",
      "36 77.24210357666016\n",
      "37 72.55087280273438\n",
      "38 68.1427230834961\n",
      "39 64.00277709960938\n",
      "40 60.1308479309082\n",
      "41 56.49887466430664\n",
      "42 53.0952033996582\n",
      "43 49.906524658203125\n",
      "44 46.91959762573242\n",
      "45 44.11970520019531\n",
      "46 41.50297164916992\n",
      "47 39.062870025634766\n",
      "48 36.77130126953125\n",
      "49 34.62575149536133\n",
      "50 32.61510467529297\n",
      "51 30.732582092285156\n",
      "52 28.968435287475586\n",
      "53 27.313982009887695\n",
      "54 25.76266098022461\n",
      "55 24.30921173095703\n",
      "56 22.946903228759766\n",
      "57 21.669231414794922\n",
      "58 20.470420837402344\n",
      "59 19.347976684570312\n",
      "60 18.293415069580078\n",
      "61 17.30330467224121\n",
      "62 16.373027801513672\n",
      "63 15.501211166381836\n",
      "64 14.681639671325684\n",
      "65 13.909913063049316\n",
      "66 13.183857917785645\n",
      "67 12.499263763427734\n",
      "68 11.854406356811523\n",
      "69 11.247345924377441\n",
      "70 10.675314903259277\n",
      "71 10.13557243347168\n",
      "72 9.626264572143555\n",
      "73 9.145630836486816\n",
      "74 8.692134857177734\n",
      "75 8.26422119140625\n",
      "76 7.859929084777832\n",
      "77 7.478569030761719\n",
      "78 7.118776321411133\n",
      "79 6.778269290924072\n",
      "80 6.455151557922363\n",
      "81 6.149427890777588\n",
      "82 5.859961986541748\n",
      "83 5.586189270019531\n",
      "84 5.326669692993164\n",
      "85 5.081073760986328\n",
      "86 4.848598003387451\n",
      "87 4.628039836883545\n",
      "88 4.418812274932861\n",
      "89 4.22011137008667\n",
      "90 4.03148889541626\n",
      "91 3.85233211517334\n",
      "92 3.6820061206817627\n",
      "93 3.5200538635253906\n",
      "94 3.366295576095581\n",
      "95 3.220104694366455\n",
      "96 3.081094980239868\n",
      "97 2.9478847980499268\n",
      "98 2.8211557865142822\n",
      "99 2.700608491897583\n",
      "100 2.585635185241699\n",
      "101 2.476266860961914\n",
      "102 2.3720734119415283\n",
      "103 2.27278733253479\n",
      "104 2.178467273712158\n",
      "105 2.0886454582214355\n",
      "106 2.0029616355895996\n",
      "107 1.9212790727615356\n",
      "108 1.8433643579483032\n",
      "109 1.7690125703811646\n",
      "110 1.6980165243148804\n",
      "111 1.630236029624939\n",
      "112 1.5654228925704956\n",
      "113 1.5034924745559692\n",
      "114 1.444340467453003\n",
      "115 1.387866735458374\n",
      "116 1.3338878154754639\n",
      "117 1.2822437286376953\n",
      "118 1.232841968536377\n",
      "119 1.1855518817901611\n",
      "120 1.1402850151062012\n",
      "121 1.0969470739364624\n",
      "122 1.0554102659225464\n",
      "123 1.0156100988388062\n",
      "124 0.9774888753890991\n",
      "125 0.9409765601158142\n",
      "126 0.9060029983520508\n",
      "127 0.8724609017372131\n",
      "128 0.8402785062789917\n",
      "129 0.8092858195304871\n",
      "130 0.7795636653900146\n",
      "131 0.751034140586853\n",
      "132 0.7236625552177429\n",
      "133 0.697391927242279\n",
      "134 0.6721824407577515\n",
      "135 0.6479606628417969\n",
      "136 0.6246941685676575\n",
      "137 0.6023442149162292\n",
      "138 0.5808628797531128\n",
      "139 0.5602169036865234\n",
      "140 0.5403794050216675\n",
      "141 0.5213167071342468\n",
      "142 0.5029921531677246\n",
      "143 0.48536738753318787\n",
      "144 0.4684107005596161\n",
      "145 0.4521111249923706\n",
      "146 0.4364219009876251\n",
      "147 0.4213317036628723\n",
      "148 0.4068053662776947\n",
      "149 0.3928261399269104\n",
      "150 0.3793690800666809\n",
      "151 0.3664127290248871\n",
      "152 0.35393786430358887\n",
      "153 0.34193798899650574\n",
      "154 0.33036914467811584\n",
      "155 0.3192187547683716\n",
      "156 0.3084754943847656\n",
      "157 0.2981330454349518\n",
      "158 0.28816646337509155\n",
      "159 0.2785727083683014\n",
      "160 0.26933375000953674\n",
      "161 0.2604302763938904\n",
      "162 0.2518427073955536\n",
      "163 0.2435620129108429\n",
      "164 0.2355814278125763\n",
      "165 0.22787995636463165\n",
      "166 0.22044798731803894\n",
      "167 0.2132810354232788\n",
      "168 0.20636311173439026\n",
      "169 0.19969020783901215\n",
      "170 0.19325482845306396\n",
      "171 0.18703718483448029\n",
      "172 0.18104007840156555\n",
      "173 0.17524453997612\n",
      "174 0.16964828968048096\n",
      "175 0.16424523293972015\n",
      "176 0.1590285748243332\n",
      "177 0.15399070084095\n",
      "178 0.14912430942058563\n",
      "179 0.14442437887191772\n",
      "180 0.13988198339939117\n",
      "181 0.13549421727657318\n",
      "182 0.13124999403953552\n",
      "183 0.12715040147304535\n",
      "184 0.12318697571754456\n",
      "185 0.11935491114854813\n",
      "186 0.11565019190311432\n",
      "187 0.11206966638565063\n",
      "188 0.10860667377710342\n",
      "189 0.10525806993246078\n",
      "190 0.10202305018901825\n",
      "191 0.09889409691095352\n",
      "192 0.09586735814809799\n",
      "193 0.09293802082538605\n",
      "194 0.09010337293148041\n",
      "195 0.0873604416847229\n",
      "196 0.0847071036696434\n",
      "197 0.08213980495929718\n",
      "198 0.07965682446956635\n",
      "199 0.07725474238395691\n",
      "200 0.07492762058973312\n",
      "201 0.07267512381076813\n",
      "202 0.07049493491649628\n",
      "203 0.06838559359312057\n",
      "204 0.06634149700403214\n",
      "205 0.06436219811439514\n",
      "206 0.062446292489767075\n",
      "207 0.06059153750538826\n",
      "208 0.0587935708463192\n",
      "209 0.05705287307500839\n",
      "210 0.05536716431379318\n",
      "211 0.05373508855700493\n",
      "212 0.052152544260025024\n",
      "213 0.05062079429626465\n",
      "214 0.049136947840452194\n",
      "215 0.04769892245531082\n",
      "216 0.0463058203458786\n",
      "217 0.04495447129011154\n",
      "218 0.043645307421684265\n",
      "219 0.04237687215209007\n",
      "220 0.04114643111824989\n",
      "221 0.03995450586080551\n",
      "222 0.03880004957318306\n",
      "223 0.03768027573823929\n",
      "224 0.03659489378333092\n",
      "225 0.03554360195994377\n",
      "226 0.03452374413609505\n",
      "227 0.03353426977992058\n",
      "228 0.032575368881225586\n",
      "229 0.03164541721343994\n",
      "230 0.03074309229850769\n",
      "231 0.02986789681017399\n",
      "232 0.029019085690379143\n",
      "233 0.028195498511195183\n",
      "234 0.0273965485394001\n",
      "235 0.02662237547338009\n",
      "236 0.025871429592370987\n",
      "237 0.025142081081867218\n",
      "238 0.024434441700577736\n",
      "239 0.023747552186250687\n",
      "240 0.023081056773662567\n",
      "241 0.022434504702687263\n",
      "242 0.021806789562106133\n",
      "243 0.021197138354182243\n",
      "244 0.020606014877557755\n",
      "245 0.02003205008804798\n",
      "246 0.019474638625979424\n",
      "247 0.018934227526187897\n",
      "248 0.01840922422707081\n",
      "249 0.01789930835366249\n",
      "250 0.017403993755578995\n",
      "251 0.016923150047659874\n",
      "252 0.016456523910164833\n",
      "253 0.0160031970590353\n",
      "254 0.015562880784273148\n",
      "255 0.01513546984642744\n",
      "256 0.014720354229211807\n",
      "257 0.01431703194975853\n",
      "258 0.013925755396485329\n",
      "259 0.013545180670917034\n",
      "260 0.013175630941987038\n",
      "261 0.01281663216650486\n",
      "262 0.012467730790376663\n",
      "263 0.012128635309636593\n",
      "264 0.011799173429608345\n",
      "265 0.011479041539132595\n",
      "266 0.011168107390403748\n",
      "267 0.01086602546274662\n",
      "268 0.01057237759232521\n",
      "269 0.010287342593073845\n",
      "270 0.010010016150772572\n",
      "271 0.009740477427840233\n",
      "272 0.009478610940277576\n",
      "273 0.009224051609635353\n",
      "274 0.008976546116173267\n",
      "275 0.00873596128076315\n",
      "276 0.008502164855599403\n",
      "277 0.008274776861071587\n",
      "278 0.00805367436259985\n",
      "279 0.007838784717023373\n",
      "280 0.007630025502294302\n",
      "281 0.007427022326737642\n",
      "282 0.0072296857833862305\n",
      "283 0.007037700153887272\n",
      "284 0.006851076614111662\n",
      "285 0.00666955066844821\n",
      "286 0.006492926273494959\n",
      "287 0.00632116524502635\n",
      "288 0.006154042202979326\n",
      "289 0.005991632118821144\n",
      "290 0.0058336709626019\n",
      "291 0.0056800455786287785\n",
      "292 0.005530708469450474\n",
      "293 0.005385328084230423\n",
      "294 0.0052438536658883095\n",
      "295 0.005106212105602026\n",
      "296 0.004972323775291443\n",
      "297 0.004842082504183054\n",
      "298 0.004715400282293558\n",
      "299 0.004592181649059057\n",
      "300 0.004472256172448397\n",
      "301 0.004355618264526129\n",
      "302 0.0042420197278261185\n",
      "303 0.004131658002734184\n",
      "304 0.004024124704301357\n",
      "305 0.003919483628123999\n",
      "306 0.003817657707259059\n",
      "307 0.0037186346016824245\n",
      "308 0.003622243879362941\n",
      "309 0.003528367727994919\n",
      "310 0.0034370282664895058\n",
      "311 0.0033481812570244074\n",
      "312 0.0032616364769637585\n",
      "313 0.0031774123199284077\n",
      "314 0.00309544475749135\n",
      "315 0.003015762660652399\n",
      "316 0.002938046120107174\n",
      "317 0.002862409455701709\n",
      "318 0.002788822166621685\n",
      "319 0.0027171699330210686\n",
      "320 0.0026473761536180973\n",
      "321 0.002579432912170887\n",
      "322 0.002513323212042451\n",
      "323 0.0024489827919751406\n",
      "324 0.0023862975649535656\n",
      "325 0.0023252193350344896\n",
      "326 0.0022658223751932383\n",
      "327 0.0022080307826399803\n",
      "328 0.0021516724955290556\n",
      "329 0.002096814103424549\n",
      "330 0.002043382963165641\n",
      "331 0.0019913306459784508\n",
      "332 0.0019406524952501059\n",
      "333 0.0018913011299446225\n",
      "334 0.0018432465149089694\n",
      "335 0.0017964182188734412\n",
      "336 0.0017508040182292461\n",
      "337 0.001706396578811109\n",
      "338 0.001663187169469893\n",
      "339 0.0016211027977988124\n",
      "340 0.0015800849068909883\n",
      "341 0.0015401256969198585\n",
      "342 0.0015011945506557822\n",
      "343 0.001463254913687706\n",
      "344 0.0014263042248785496\n",
      "345 0.001390322926454246\n",
      "346 0.0013552795862779021\n",
      "347 0.0013211170444265008\n",
      "348 0.0012878385605290532\n",
      "349 0.0012554213171824813\n",
      "350 0.0012238684576004744\n",
      "351 0.0011931274784728885\n",
      "352 0.0011631487868726254\n",
      "353 0.0011339503107592463\n",
      "354 0.0011054981732740998\n",
      "355 0.0010777906281873584\n",
      "356 0.0010507676051929593\n",
      "357 0.0010244473814964294\n",
      "358 0.0009987998055294156\n",
      "359 0.00097382947569713\n",
      "360 0.0009494827827438712\n",
      "361 0.0009257435449399054\n",
      "362 0.000902607396710664\n",
      "363 0.0008801089134067297\n",
      "364 0.0008581369183957577\n",
      "365 0.0008367350674234331\n",
      "366 0.0008158780983649194\n",
      "367 0.0007955483160912991\n",
      "368 0.0007757404237054288\n",
      "369 0.0007564350962638855\n",
      "370 0.000737626978661865\n",
      "371 0.0007192868506535888\n",
      "372 0.0007014275179244578\n",
      "373 0.0006839964189566672\n",
      "374 0.0006670261500403285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.0006505012279376388\n",
      "376 0.0006343786371871829\n",
      "377 0.0006186614627949893\n",
      "378 0.0006033276440575719\n",
      "379 0.0005883832345716655\n",
      "380 0.0005738206673413515\n",
      "381 0.0005596213741227984\n",
      "382 0.0005457888473756611\n",
      "383 0.0005322962533682585\n",
      "384 0.0005191444652155042\n",
      "385 0.000506328884512186\n",
      "386 0.0004938290221616626\n",
      "387 0.00048163760220631957\n",
      "388 0.0004697689728345722\n",
      "389 0.0004582055553328246\n",
      "390 0.00044691533548757434\n",
      "391 0.00043590739369392395\n",
      "392 0.0004251690406817943\n",
      "393 0.0004147063591517508\n",
      "394 0.00040450665983371437\n",
      "395 0.0003945553908124566\n",
      "396 0.0003848606429528445\n",
      "397 0.00037539892946369946\n",
      "398 0.0003661849768832326\n",
      "399 0.00035720854066312313\n",
      "400 0.000348439411027357\n",
      "401 0.0003398970584385097\n",
      "402 0.00033156739664264023\n",
      "403 0.0003234421892557293\n",
      "404 0.0003155224258080125\n",
      "405 0.00030779733788222075\n",
      "406 0.0003002593875862658\n",
      "407 0.00029291390092112124\n",
      "408 0.00028574312455020845\n",
      "409 0.0002787590492516756\n",
      "410 0.000271946337306872\n",
      "411 0.00026530082686804235\n",
      "412 0.00025882109184749424\n",
      "413 0.0002525025047361851\n",
      "414 0.00024633959401398897\n",
      "415 0.00024033462977968156\n",
      "416 0.00023447422427125275\n",
      "417 0.00022875398281030357\n",
      "418 0.00022317911498248577\n",
      "419 0.00021774203923996538\n",
      "420 0.00021243662922643125\n",
      "421 0.00020726659568026662\n",
      "422 0.0002022238913923502\n",
      "423 0.00019730582425836474\n",
      "424 0.00019250763580203056\n",
      "425 0.0001878279581433162\n",
      "426 0.00018326277495361865\n",
      "427 0.00017881377425510436\n",
      "428 0.0001744656328810379\n",
      "429 0.00017023469263222069\n",
      "430 0.0001660993875702843\n",
      "431 0.0001620692346477881\n",
      "432 0.00015813524078112096\n",
      "433 0.00015429955965373665\n",
      "434 0.0001505605468992144\n",
      "435 0.00014691019896417856\n",
      "436 0.00014335215382743627\n",
      "437 0.00013987701095174998\n",
      "438 0.0001364911295240745\n",
      "439 0.00013318308629095554\n",
      "440 0.00012996031728107482\n",
      "441 0.00012681707448791713\n",
      "442 0.0001237515825778246\n",
      "443 0.00012076242273906246\n",
      "444 0.0001178424499812536\n",
      "445 0.0001149943345808424\n",
      "446 0.00011221617023693398\n",
      "447 0.00010950590512948111\n",
      "448 0.00010686153109418228\n",
      "449 0.00010428134555695578\n",
      "450 0.00010176430077990517\n",
      "451 9.930722444551066e-05\n",
      "452 9.691028390079737e-05\n",
      "453 9.457595297135413e-05\n",
      "454 9.22925173654221e-05\n",
      "455 9.006822801893577e-05\n",
      "456 8.79026047186926e-05\n",
      "457 8.57846753206104e-05\n",
      "458 8.371831063413993e-05\n",
      "459 8.170259388862178e-05\n",
      "460 7.973532046889886e-05\n",
      "461 7.781643944326788e-05\n",
      "462 7.594288035761565e-05\n",
      "463 7.411641854560003e-05\n",
      "464 7.233369251480326e-05\n",
      "465 7.059406925691292e-05\n",
      "466 6.889844371471554e-05\n",
      "467 6.724218837916851e-05\n",
      "468 6.56265692668967e-05\n",
      "469 6.405053864000365e-05\n",
      "470 6.251001468626782e-05\n",
      "471 6.100907557993196e-05\n",
      "472 5.9547543060034513e-05\n",
      "473 5.8119461755268276e-05\n",
      "474 5.6722430599620566e-05\n",
      "475 5.536193202715367e-05\n",
      "476 5.403558679972775e-05\n",
      "477 5.274035356706008e-05\n",
      "478 5.1476214139256626e-05\n",
      "479 5.0242502766195685e-05\n",
      "480 4.903853914584033e-05\n",
      "481 4.78645451948978e-05\n",
      "482 4.6718425437575206e-05\n",
      "483 4.559816079563461e-05\n",
      "484 4.4506497943075374e-05\n",
      "485 4.344211265561171e-05\n",
      "486 4.240254929754883e-05\n",
      "487 4.13884044974111e-05\n",
      "488 4.039857958559878e-05\n",
      "489 3.943321280530654e-05\n",
      "490 3.848948108498007e-05\n",
      "491 3.7569927371805534e-05\n",
      "492 3.667309647426009e-05\n",
      "493 3.5794582800008357e-05\n",
      "494 3.4940730984089896e-05\n",
      "495 3.410521094338037e-05\n",
      "496 3.3291358704445884e-05\n",
      "497 3.249421206419356e-05\n",
      "498 3.171973003190942e-05\n",
      "499 3.09631614072714e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    # print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FizzBuzz\n",
    "\n",
    "FizzBuzz是一个简单的小游戏。游戏规则如下：从1开始往上数数，当遇到3的倍数的时候，说fizz，当遇到5的倍数，说buzz，当遇到15的倍数，就说fizzbuzz，其他情况下则正常数数。\n",
    "\n",
    "我们可以写一个简单的小程序来决定要返回正常数值还是fizz, buzz 或者 fizzbuzz。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "buzz\n",
      "fizz\n",
      "fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return 3\n",
    "    elif i % 5  == 0: return 2\n",
    "    elif i % 3  == 0: return 1\n",
    "    else:             return 0\n",
    "    \n",
    "def fizz_buzz_decode(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n",
    "\n",
    "print(fizz_buzz_decode(1, fizz_buzz_encode(1)))\n",
    "print(fizz_buzz_decode(2, fizz_buzz_encode(2)))\n",
    "print(fizz_buzz_decode(5, fizz_buzz_encode(5)))\n",
    "print(fizz_buzz_decode(12, fizz_buzz_encode(12)))\n",
    "print(fizz_buzz_decode(15, fizz_buzz_encode(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先定义模型的输入与输出(训练数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NUM_DIGITS = 10\n",
    "\n",
    "# Represent each input by an array of its binary digits.\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])   # 2 变成 0100000000，顺序有反，但是无所谓\n",
    "\n",
    "trX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])   # 10位二进制\n",
    "trY = torch.LongTensor([fizz_buzz_encode(i) for i in range(101, 2 ** NUM_DIGITS)])        # 0、1、2或3，注意这里是LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([923])\n"
     ]
    }
   ],
   "source": [
    "print (trY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们用PyTorch定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "NUM_HIDDEN = 100\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了让我们的模型学会FizzBuzz这个游戏，我们需要定义一个损失函数，和一个优化算法。\n",
    "- 这个优化算法会不断优化（降低）损失函数，使得模型的在该任务上取得尽可能低的损失值。\n",
    "- 损失值低往往表示我们的模型表现好，损失值高表示我们的模型表现差。\n",
    "- 由于FizzBuzz游戏本质上是一个分类问题，我们选用Cross Entropy Loss函数。\n",
    "- 优化函数我们选用Stochastic Gradient Descent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是模型的训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.217276930809021\n",
      "Epoch: 1000 Loss: 0.5164196491241455\n",
      "Epoch: 2000 Loss: 0.12073612958192825\n",
      "Epoch: 3000 Loss: 0.05368366837501526\n",
      "Epoch: 4000 Loss: 0.03037494607269764\n",
      "Epoch: 5000 Loss: 0.01970599591732025\n",
      "Epoch: 6000 Loss: 0.014038625173270702\n",
      "Epoch: 7000 Loss: 0.010669323615729809\n",
      "Epoch: 8000 Loss: 0.008475510403513908\n",
      "Epoch: 9000 Loss: 0.0069842347875237465\n"
     ]
    }
   ],
   "source": [
    "# Start training it\n",
    "BATCH_SIZE = 128\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(trX), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE  \n",
    "        batchX = trX[start:end]                           # 竟然没有越界\n",
    "        batchY = trY[start:end]\n",
    "\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)                    # 注意这两个的维度：https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Find loss on training data\n",
    "    loss = loss_fn(model(trX), trY).item()\n",
    "    if (epoch%1000==0):\n",
    "        print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们用训练好的模型尝试在1到100这些数字上玩FizzBuzz游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', 'fizz', '22', 'fizz', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', 'buzz', 'fizz', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', 'fizz', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', 'fizzbuzz', '91', '92', 'fizz', '94', 'buzz', 'buzz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "# Output now\n",
    "testX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "with torch.no_grad():\n",
    "    testY = model(testX)\n",
    "predictions = zip(range(1, 101), list(testY.max(1)[1].data.tolist()))\n",
    "\n",
    "print([fizz_buzz_decode(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 6.3046,  9.3019, 10.6227,  5.2429,  7.0908,  3.6957,  7.6848,  6.7376,\n",
       "         8.8358,  7.0454,  8.9054,  6.5258, 11.1373,  8.1076,  6.8863,  8.8270,\n",
       "         3.7558, 10.3897,  4.4458,  7.7029,  8.3061,  8.3061,  3.9069,  8.4617,\n",
       "         5.7278,  8.7762, 10.1094, 12.1703,  7.7449,  5.7961,  6.3213,  5.9292,\n",
       "         5.7895,  5.8037,  4.9718,  6.1873,  8.1670,  5.9121,  5.9668,  6.4944,\n",
       "         6.8159,  6.3422,  9.4932,  6.0991,  5.6619,  6.2464,  4.7406,  9.2211,\n",
       "         6.8329,  6.7497,  5.2758,  8.7305,  5.2762,  4.1747,  7.3745,  8.2549,\n",
       "         9.0619,  8.5187,  8.2312,  5.6800,  8.1132,  8.1878,  7.8953,  6.5253,\n",
       "        10.1245,  4.0647,  6.1315,  8.3083,  8.7441,  8.8598,  7.2043,  7.6296,\n",
       "        10.1160,  5.1331,  4.9104,  7.5242,  7.2350,  8.3422,  6.0486, 10.9966,\n",
       "         4.0352,  8.2559,  5.0356,  7.3801, 10.1361,  8.6810,  6.2584, 11.9410,\n",
       "         9.7580,  6.9164,  5.4353,  7.1868,  8.0638,  6.0965,  2.9596,  2.9086,\n",
       "        10.0201,  6.9633,  5.5360, 10.0560]),\n",
       "indices=tensor([0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 1, 1,\n",
       "        2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1,\n",
       "        0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 2, 1, 2, 0, 1,\n",
       "        0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 2,\n",
       "        0, 0, 1, 2]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.max(1)                 # 注意这里有两部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])))\n",
    "testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考资料 reference](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
